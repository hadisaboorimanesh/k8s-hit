groups:

  - name: Container
    rules:
    - alert: Container restarted
      annotations:
        summary: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} was restarted\n"
        description: "Namespace: {{$labels.namespace}}  Pod name: {{$labels.pod}}  Container name: {{$labels.container}}\n" 
        resolved: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
      expr: |
        sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[1m])) by (pod,namespace,container) > 0
      for: 0m
      labels:
        severity: critical
    # - alert: High Memory Usage of Container 
    #   annotations: 
    #     summary: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of Memory Limit.\n"
    #     description: "Cluster Name:  {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} Pod name: {{$labels.pod}} Container name: {{$labels.container}}\n" 
    #     resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
    #   expr: |
    #     ((( sum(container_memory_working_set_bytes{image!="",container!="POD", namespace!="kube-system"}) by (namespace,container,pod)  / sum(container_spec_memory_limit_bytes{image!="",container!="POD",namespace!="kube-system"}) by (namespace,container,pod) ) * 100 ) < +Inf ) > 75
    #   for: 5m
    #   labels: 
    #     severity: critical

    - alert: High Memory Usage of Deployment 
      annotations: 
        summary: "Deployment named {{$labels.container}} in stage is using more than 75% of Memory Limit.\n"
        description: "Namespace: stage  Deployment name: {{$labels.container}}\n" 
        resolved: "Deployment named {{$labels.container}}  in stage is resolved"
      expr: |
        sum (container_memory_working_set_bytes{origin_prometheus=~"",container !="",container!="POD",namespace=~"stage",container!~"^mattermost.*$",container!="metrics",container!="mongodb-exporter"}) by (container)/ sum(container_spec_memory_limit_bytes{origin_prometheus=~"",container !="",container!="POD",namespace=~"stage",container!~"^mattermost.*$",container!="metrics",container!="mongodb-exporter"}) by (container) * 100 > 75
      for: 5m
      labels: 
        severity: critical

    - alert: High CPU Usage of Container 
      annotations: 
        summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of CPU Limit
        description: "Cluster Name:  {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} Pod name: {{$labels.pod}} Container name: {{$labels.container}}" 
        resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
      expr: |
        ((sum(irate(container_cpu_usage_seconds_total{image!="",container!="POD", namespace!="kube-system"}[30s])) by (namespace,container,pod) / sum(container_spec_cpu_quota{image!="",container!="POD", namespace!="kube-system"} / container_spec_cpu_period{image!="",container!="POD", namespace!="kube-system"}) by (namespace,container,pod) ) * 100)  > 75
      for: 5m
      labels: 
        severity: critical

    - alert: High CPU Usage of Deployment 
      annotations: 
        summary: Deployment named {{$labels.container}}  in stage is using more than 90% of CPU Limit
        description: "Namespace: stage  Deployment name: {{$labels.container}}" 
        resolved: "Deployment named {{$labels.container}} in  stage is resolved"
      expr: |
        sum(irate(container_cpu_usage_seconds_total{origin_prometheus=~"",container !="",container!="POD",namespace=~"stage"}[2m])) by (container) / (sum(container_spec_cpu_quota{origin_prometheus=~"",container !="",container!="POD",namespace=~"stage"}/100000) by (container)) *  100 > 90
      for: 5m
      labels: 
        severity: critical


    - alert: High Persistent Volume Usage
      annotations:
        summary: "Persistent Volume named {{$labels.persistentvolumeclaim}} in {{$labels.namespace}} is using more than 75% used. \n"
        description: "Cluster Name: {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} PVC name: {{$labels.persistentvolumeclaim}}\n"
        resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
      expr: |
        ((((sum(kubelet_volume_stats_used_bytes{})  by (namespace,persistentvolumeclaim))  / (sum(kubelet_volume_stats_capacity_bytes{}) by (namespace,persistentvolumeclaim)))*100) < +Inf ) > 75
      for: 5m
      labels:
        severity: critical
  - name: Nodes
    rules:
    #  - alert: High Node Memory Usage
    #    annotations:
    #      summary: "Node {{$labels.kubernetes_io_hostname}} has more than 80% memory used. Plan Capcity\n"
    #      description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
    #      resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
    #    expr: |
    #      (sum (container_memory_working_set_bytes{id="/",container!="POD"}) by (kubernetes_io_hostname) / sum (machine_memory_bytes{}) by (kubernetes_io_hostname) * 100) > 80
    #    for: 5m
    #    labels:
    #      severity: critical
    #      type: os-metrics
    - alert: High Node CPU Usage
      annotations:
        summary: "Node {{$labels.kubernetes_io_hostname}} has more than 80% allocatable cpu used. Plan Capacity.\n"
        description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
        resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
      expr: |
        (sum(rate(container_cpu_usage_seconds_total{id="/", container!="POD"}[1m])) by (kubernetes_io_hostname) / sum(machine_cpu_cores) by (kubernetes_io_hostname)  * 100) > 80
      for: 5m
      labels:
        severity: critical
        type: os-metrics
    - alert: High Node Disk Usage
      annotations:
        summary: "Node {{$labels.kubernetes_io_hostname}} has more than 85% disk used. Plan Capacity.\n"
        description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
        resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
      expr: |
        (sum(container_fs_usage_bytes{device=~"^/dev/[sv]d[a-z][1-9]$",id="/",container!="POD"}) by (kubernetes_io_hostname) / sum(container_fs_limit_bytes{container!="POD",device=~"^/dev/[sv]d[a-z][1-9]$",id="/"}) by (kubernetes_io_hostname)) * 100 > 85
      for: 5m
      labels:
        severity: critical
        type: os-metrics
  - name: windows
    rules:
    #  - alert: WindowsServerCpuUsage
    #    expr: 100 - (avg by (instance) (rate(windows_cpu_time_total{mode="idle"}[2m])) * 100) > 85
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Windows Server CPU Usage (instance {{ $labels.instance }})\n"
    #      description: "CPU Usage is more than 85% VALUE = {{ $value }}\n"
    #      resolved: "Windows Server CPU Usage (instance {{ $labels.instance }}) is resolved"
    #  - alert: WindowsServerMemoryUsage
    #    expr: 100 - ((windows_os_physical_memory_free_bytes / windows_cs_physical_memory_bytes) * 100) > 90
    #    for: 2m
    #    labels:
    #      severity: critical
    #      type: os-metrics
    #    annotations:
    #      summary: "Windows Server memory Usage (instance {{ $labels.instance }})\n"
    #      description: "Memory usage is more than 90% VALUE = {{ $value }}\n"
    #      resolved: "Windows Server memory Usage (instance {{ $labels.instance }}) is resolved"
    - alert: WindowsServerDiskSpaceUsage
      expr:  |
        (
          100.0 - 100 * (
          (windows_logical_disk_free_bytes{volume !~ "HarddiskVolume.*"} / 1024 / 1024) / 
          (windows_logical_disk_size_bytes{volume !~ "HarddiskVolume.*"} / 1024 / 1024)
          )  > 85
          )
          unless on(instance, volume) 
          (
          windows_logical_disk_free_bytes{instance="172.31.58.34:9182", volume="F:\\Data01"} or
          windows_logical_disk_free_bytes{instance="172.31.58.34:9182", volume="F:\\Data02"} or 
          windows_logical_disk_free_bytes{instance="172.31.58.35:9182", volume="F:\\Data01"} or
          windows_logical_disk_free_bytes{instance="172.31.58.35:9182", volume="F:\\Data02"} 
        )
      for: 2m
      labels:
        severity: critical
        type: os-metrics
      annotations:
        summary: "Windows Server disk Space Usage (volume {{ $labels.volume }})\n"
        description: "Disk usage is more than 85% VALUE = {{ $value }}\n"
        resolved: "Windows Server disk Space Usage (volume {{ $labels.volume }}) is resolved"
    #  - alert: WindowsServerServiceStatus
    #    expr: windows_service_status{status="ok"} != 1
    #    for: 1m
    #    labels:
    #      severity: critical
        
    #      type: os-metrics
    #    annotations:
    #      summary: "Windows Server service Status (instance {{ $labels.instance }})\n"
    #      description: "Windows Service state is not OK VALUE = {{ $value }}\n"
    #  - alert: WindowsServerCollectorError
    #    expr: windows_exporter_collector_success == 0
    #    for: 0m
    #    labels:
    #      severity: critical
    #      type: os-metrics
    #    annotations:
    #      summary: "Windows Server collector Error (instance {{ $labels.instance }})\n"
    #      description: "Collector {{ $labels.collector }} was not successful VALUE = {{ $value }}\n"
  - name: node_exporter_alerts
    rules:
    - alert: Windows server Node down
      expr: up{job="windows-servers"} == 0
      for: 2m
      labels:
        severity: warning
        type: os-metrics
      annotations:
        summary: "Node {{ $labels.instance }} is down\n"
        description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }} for more than 2 minutes. Node seems down.\n"
        resolved: "Node {{ $labels.instance }} is resolved and Up Now"
    - alert: Kubernetes Node down
      expr: sum(up{job="kubernetes-nodes-cadvisor"} == 0) by (environment,instance,job)
      for: 2m
      labels:
        severity: warning      
        type: os-metrics
      annotations:
        summary: "Node {{ $labels.instance }} is down\n"
        description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }}   {{ $labels.environment }} for more than 2 minutes. Node seems down.\n"   
        resolved: "Node {{ $labels.instance }} is resolved and Up Now"
    - alert: Node down
      expr: up{job="linux-servers"} == 0
      for: 2m
      labels:
        severity: warning
        type: os-metrics
      annotations:
        summary: "Node {{ $labels.instance }} is down\n"
        description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }} for more than 2 minutes. Node seems down.\n"
        resolved: "Node {{ $labels.instance }} is resolved and Up Now"
    #  - alert: HostOutOfMemory
    #    expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host out of memory (instance {{ $labels.instance }})\n"
    #      description: "Node memory is filling up (< 10% left) VALUE = {{ $value }}\n"
    #      resolved: "Host out of memory (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostMemoryUnderMemoryPressure
    #    expr: rate(node_vmstat_pgmajfault[1m]) > 2000
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host under memory pressure (instance {{ $labels.instance }})\n"
    #      description: "The node is under heavy memory pressure. High rate of major page faults VALUE = {{ $value }}\n"
    #      resolved: "Host under memory pressure (instance {{ $labels.instance }}) is resolved"
    # - alert: HostUnusualNetworkThroughputIn
    #   expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
    #   for: 5m
    #   labels:
    #     severity: warning
    #     type: os-metrics
    #   annotations:
    #     summary: "Host unusual network throughput in (instance {{ $labels.instance }})\n"
    #     description: "Host network interfaces are probably receiving too much data (> 100 MB/s) VALUE = {{ $value }}\n"
    # - alert: HostUnusualNetworkThroughputOut
    #   expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
    #   for: 5m
    #   labels:
    #     severity: warning
    #     type: os-metrics
    #   annotations:
    #     summary: "Host unusual network throughput out (instance {{ $labels.instance }})\n"
    #     description: "Host network interfaces are probably sending too much data (> 100 MB/s) VALUE = {{ $value }}\n"
    #  - alert: HostUnusualDiskReadRate
    #    expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 60
    #    for: 5m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host unusual disk read rate (instance {{ $labels.instance }})\n"
    #      description: "Disk is probably reading too much data (> 60 MB/s) VALUE = {{ $value }}\n"
    # - alert: HostUnusualDiskWriteRate
    #   expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 80
    #   for: 2m
    #   labels:
    #     severity: warning
    #     type: os-metrics
    #   annotations:
    #     summary: "Host unusual disk write rate (instance {{ $labels.instance }})\n"
    #     description: "Disk is probably writing too much data (> 80 MB/s) VALUE = {{ $value }}\n"
    - alert: HostOutOfDiskSpace
      expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      for: 2m
      labels:
        severity: warning
        type: os-metrics
      annotations:
        summary: "Host out of disk space (instance {{ $labels.instance }})\n"
        description: "Disk is almost full (< 10% left) VALUE = {{ $value }}\n"
        resolved: "Host out of disk space (instance {{ $labels.instance }}) is resolved"
    # - alert: HostDiskWillFillIn24Hours
    #   expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
    #   for: 2m
    #   labels:
    #     severity: warning
    #     type: os-metrics
    #   annotations:
    #     summary: "Host disk will fill in 24 hours (instance {{ $labels.instance }})\n"
    #     description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate VALUE = {{ $value }}\n"
    #  - alert: HostOutOfInodes
    #    expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host out of inodes (instance {{ $labels.instance }})\n"
    #      description: "Disk is almost running out of available inodes (< 10% left) VALUE = {{ $value }}\n"
    #      resolved: "Host out of inodes (instance {{ $labels.instance }}) is resolved"
    # - alert: HostInodesWillFillIn24Hours
    #   expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
    #   for: 2m
    #   labels:
    #     severity: warning
    #     type: os-metrics
    #   annotations:
    #     summary: "Host inodes will fill in 24 hours (instance {{ $labels.instance }})\n"
    #     description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate VALUE = {{ $value }}\n"
    #  - alert: HostUnusualDiskReadLatency
    #    expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host unusual disk read latency (instance {{ $labels.instance }})\n"
    #      description: "Disk latency is growing (read operations > 100ms) VALUE = {{ $value }}\n"
    #  - alert: HostUnusualDiskWriteLatency
    #    expr: rate(node_disk_write_time_seconds_totali{device!~"mmcblk.+"}[1m]) / rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0.1 and rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host unusual disk write latency (instance {{ $labels.instance }})\n"
    #      description: "Disk latency is growing (write operations > 100ms) VALUE = {{ $value }}\n"
    #  - alert: HostHighCpuLoad
    #    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host high CPU load (instance {{ $labels.instance }})\n"
    #      description: "CPU load is > 80% VALUE = {{ $value }}\n"
    #      resolved: "Host CPU load (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostCpuStealNoisyNeighbor
    #    expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host CPU steal noisy neighbor (instance {{ $labels.instance }})\n"
    #      description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit. VALUE = {{ $value }}\n"
    #      resolved: "Host CPU steal noisy neighbor (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostContextSwitching
    #    expr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode="idle"})) > 20000
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host context switching (instance {{ $labels.instance }})\n"
    #      description: "Context switching is growing on node (> 20000 / s) VALUE = {{ $value }}\n"
    #      resolved: "Host context switching (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostSwapIsFillingUp
    #    expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host swap is filling up (instance {{ $labels.instance }})\n"
    #      description: "Swap is filling up (>80%) VALUE = {{ $value }}\n"
    #      resolved: "Host swap  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostSystemdServiceCrashed
    #    expr: node_systemd_unit_state{state="failed"} == 1
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host SystemD service crashed (instance {{ $labels.instance }})\n"
    #      description: "SystemD service crashed VALUE = {{ $value }}\n"
    #      resolved: "Host SystemD service  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostPhysicalComponentTooHot
    #    expr: node_hwmon_temp_celsius > 75
    #    for: 5m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host physical component too hot (instance {{ $labels.instance }})\n"
    #      description: "Physical hardware component too hot VALUE = {{ $value }}\n"
    #      resolved: "Host physical component (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostNodeOvertemperatureAlarm
    #    expr: node_hwmon_temp_crit_alarm_celsius == 1
    #    for: 0m
    #    labels:
    #      severity: critical
    #      type: os-metrics
    #    annotations:
    #      summary: "Host node overtemperature alarm (instance {{ $labels.instance }})\n"
    #      description: "Physical node temperature alarm triggered VALUE = {{ $value }}\n"
    #      resolved: "Host node overtemperature alarm (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostRaidArrayGotInactive
    #    expr: node_md_state{state="inactive"} > 0
    #    for: 0m
    #    labels:
    #      severity: critical
    #      type: os-metrics
    #    annotations:
    #      summary: "Host RAID array got inactive (instance {{ $labels.instance }})\n"
    #      description: "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. VALUE = {{ $value }}\n"
    #      resolved: "Host RAID  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostRaidDiskFailure
    #    expr: node_md_disks{state="failed"} > 0
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host RAID disk failure (instance {{ $labels.instance }})\n"
    #      description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap VALUE = {{ $value }}\n"
    #      resolved: "Host RAID disk  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostOomKillDetected
    #    expr: increase(node_vmstat_oom_kill[1m]) > 0
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host OOM kill detected (instance {{ $labels.instance }})\n"
    #      description: "OOM kill detected VALUE = {{ $value }}\n"
    #      resolved: "Host  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostEdacCorrectableErrorsDetected
    #    expr: increase(node_edac_correctable_errors_total[1m]) > 0
    #    for: 0m
    #    labels:
    #      severity: info
    #      type: os-metrics
    #    annotations:
    #      summary: "Host EDAC Correctable Errors detected (instance {{ $labels.instance }})\n"
    #      description: Instance has had {{ printf "%.0f" $value }} correctable memory errors reported by EDAC in the last 5 minutes. VALUE = {{ $value }}\n
    #      resolved: "Host  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostEdacUncorrectableErrorsDetected
    #    expr: node_edac_uncorrectable_errors_total > 0
    #    for: 0m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})\n"
    #      description: Instance has had {{ printf "%.0f" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes. VALUE = {{ $value }}\n
    #      resolved: "Host  (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostNetworkReceiveErrors
    #    expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host Network Receive Errors (instance {{ $labels.instance }}:{{ $labels.device }})\n"
    #      description: Instance interface has encountered {{ printf "%.0f" $value }} receive errors in the last five minutes. VALUE = {{ $value }}\n
    #      resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostNetworkTransmitErrors
    #    expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host Network Transmit Errors (instance {{ $labels.instance }}:{{ $labels.device }})\n"
    #      description: Instance has encountered {{ printf "%.0f" $value }} transmit errors in the last five minutes. VALUE = {{ $value }}\n
    #      resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostNetworkInterfaceSaturated
    #    expr: (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8
    #    for: 1m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host Network Interface Saturated (instance {{ $labels.instance }}:{{ $labels.interface }})\n"
    #      description: "The network interface is getting overloaded. VALUE = {{ $value }}\n"
    #  - alert: HostConntrackLimit
    #    expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8
    #    for: 5m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host conntrack limit (instance {{ $labels.instance }})\n"
    #      description: "The number of conntrack is approching limit VALUE = {{ $value }}\n"
    #      resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
    #  - alert: HostClockSkew
    #    expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host clock skew (instance {{ $labels.instance }})\n"
    #      description: "Clock skew detected. Clock is out of sync. VALUE = {{ $value }}\n"
    #  - alert: HostClockNotSynchronising
    #    expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds > 16
    #    for: 2m
    #    labels:
    #      severity: warning
    #      type: os-metrics
    #    annotations:
    #      summary: "Host clock not synchronising (instance {{ $labels.instance }})\n"
    #      description: "Clock not synchronising. VALUE = {{ $value }}\n"
  - name: MySql_alerts
    rules:
    - alert: MysqlDown
      expr: mysql_up == 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "MySQL down (instance {{ $labels.instance }})\n"
        description: "MySQL instance is down on {{ $labels.instance }} VALUE = {{ $value }}\n"
        resolved: "MySQL (instance {{ $labels.instance }}) is resolved and up"
    - alert: MysqlTooManyConnections(>80%)
      expr: max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections * 100 > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "MySQL too many connections (> 80%) (instance {{ $labels.instance }})\n"
        description: "More than 80% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }}\n"
        resolved: "MySQL connections (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlHighThreadsRunning
      expr: max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections * 100 > 60
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "MySQL high threads running (instance {{ $labels.instance }})\n"
        description: "More than 60% of MySQL connections are in running state on {{ $labels.instance }} VALUE = {{ $value }}\n"
        resolved: "MySQL threads  (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlSlaveIoThreadNotRunning
      expr: ( mysql_slave_status_slave_io_running and ON (instance) mysql_slave_status_master_server_id > 0 ) == 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "MySQL Slave IO thread not running (instance {{ $labels.instance }})\n"
        description: "MySQL Slave IO thread not running on {{ $labels.instance }} VALUE = {{ $value }}\n"
        resolved: "MySQL Slave IO thread  (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlSlaveSqlThreadNotRunning
      expr: ( mysql_slave_status_slave_sql_running and ON (instance) mysql_slave_status_master_server_id > 0) == 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "MySQL Slave SQL thread not running (instance {{ $labels.instance }})\n"
        description: "MySQL Slave SQL thread not running on {{ $labels.instance }} VALUE = {{ $value }}\n"
        resolved: "MySQL Slave SQL thread  (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlSlaveReplicationLag
      expr: ( (mysql_slave_status_seconds_behind_master - mysql_slave_status_sql_delay) and ON (instance) mysql_slave_status_master_server_id > 0 ) > 30
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "MySQL Slave replication lag (instance {{ $labels.instance }})\n"
        description: "MySQL replication lag on {{ $labels.instance }} VALUE = {{ $value }}\n" 
        resolved: "MySQL Slave replication lag (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlSlowQueries
      expr: increase(mysql_global_status_slow_queries[1m]) > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "MySQL slow queries (instance {{ $labels.instance }})\n"
        description: "MySQL server mysql has some new slow query. VALUE = {{ $value }}\n"
        resolved: "MySQL slow queries (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlInnodbLogWaits
      expr: rate(mysql_global_status_innodb_log_waits[15m]) > 10
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "MySQL InnoDB log waits (instance {{ $labels.instance }})\n"
        description: "MySQL innodb log writes stalling VALUE = {{ $value }}\n"
        resolved: "MySQL InnoDB log waits (instance {{ $labels.instance }}) is resolved"
    - alert: MysqlRestarted
      expr: mysql_global_status_uptime < 60
      for: 0m
      labels:
        severity: info
      annotations:
        summary: "MySQL restarted (instance {{ $labels.instance }})\n"
        description: "MySQL has just been restarted, less than one minute ago on {{ $labels.instance }}. VALUE = {{ $value }}\n"
        resolved: "MySQL  (instance {{ $labels.instance }}) is resolved"
  - name: Minio_alerts
    rules:
    - alert: MinioClusterDiskOffline
      expr: minio_cluster_disk_offline_total > 0
      for: 0m
      labels:
        severity: critical
        type:  minio
      annotations:
        summary: "Minio cluster disk offline (instance {{ $labels.instance }})\n"
        description: "Minio cluster disk is offline VALUE = {{ $value }}\n"
        resolved: "Minio cluster disk offline (instance {{ $labels.instance }}) is resolved"
    - alert: MinioNodeDiskOffline
      expr: minio_cluster_nodes_offline_total > 0
      for: 0m
      labels:
        severity: critical
        type:  minio
      annotations:
        summary: "Minio node disk offline (instance {{ $labels.instance }})\n"
        description: "Minio cluster node disk is offline VALUE = {{ $value }}\n"
        resolved: "Minio node disk offline (instance {{ $labels.instance }}) is resolved"
    - alert: MinioDiskSpaceUsage
      expr: disk_storage_available / disk_storage_total * 100 < 10
      for: 0m
      labels:
        severity: warning
        type:  minio
      annotations:
        summary: "Minio disk space usage (instance {{ $labels.instance }})\n"
        description: "Minio available free space is low (< 10%) VALUE = {{ $value }}\n"
        resolved: "Minio disk space usage (instance {{ $labels.instance }}) is resolved"
    - alert: MinioClient
      expr: mc_mirror_failed_s3ops{} > 10
      for: 5m
      labels:
        severity: critical
        type:  minio
      annotations:
        summary: "Minio client sync Failed for VALUE = {{ $value }}\n"
        description: "Some photos have not been sent to Arvan  VALUE = {{ $value }}\n"       
        resolved: "Minio client is synced"
  - name: Mongodb_alerts
    rules:
    - alert: MongodbDown
      expr: mongodb_up == 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "MongoDB Down (instance {{ $labels.instance }})\n"
        description: "MongoDB instance is down VALUE = {{ $value }}\n"
        resolved: "MongoDB (instance {{ $labels.instance }}) is resolved and up"
  - name: RabbitMq_alerts
    rules:
    - alert: RabbitmqMemoryHigh
      expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 > 90
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "RabbitMQ memory high (instance {{ $labels.instance }})\n"
        description: "A node use more than 90% of allocated RAM VALUE = {{ $value }}\n"
        resolved: "RabbitMQ memory high (instance {{ $labels.instance }}) is resolved"
    - alert: RabbitmqFileDescriptorsUsage
      expr: rabbitmq_process_open_fds / rabbitmq_process_max_fds * 100 > 90
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "RabbitMQ file descriptors usage (instance {{ $labels.instance }})\n"
        description: "A node use more than 90% of file descriptors VALUE = {{ $value }}\n"
        resolved: "RabbitMQ file descriptors usage (instance {{ $labels.instance }}) is resolved"
    - alert: RabbitmqTooManyUnackMessages
      expr: sum(rabbitmq_queue_messages_unacked) BY (queue) > 1000
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "RabbitMQ too many unack messages (instance {{ $labels.instance }})\n"
        description: "Too many unacknowledged messages VALUE = {{ $value }}\n"
        resolved: "RabbitMQ unack messages (instance {{ $labels.instance }}) is resolved"
    - alert: RabbitmqTooManyConnections
      expr: rabbitmq_connections > 1000
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "RabbitMQ too many connections (instance {{ $labels.instance }})\n"
        description: "The total connections of a node is too high VALUE = {{ $value }}\n"
        resolved: "RabbitMQ connections (instance {{ $labels.instance }}) is resolved"
    - alert: RabbitmqNoQueueConsumer
      expr: rabbitmq_queue_consumers < 1
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "RabbitMQ no queue consumer (instance {{ $labels.instance }})\n"
        description: "A queue has less than 1 consumer VALUE = {{ $value }}\n"
        resolved: "RabbitMQ queue consumer (instance {{ $labels.instance }}) is resolved"
  - name: Redis_alerts
    rules:
    - alert: RedisDown
      expr: redis_up == 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis down (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis instance is on (namespace {{ $labels.kubernetes_namespace }}) down VALUE = {{ $value }}\n"
        resolved: "Redis (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved and up"
    - alert: RedisMissingMaster
      expr: (redis_instance_info{role="master"})  < 1
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis missing master (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis cluster on (namespace {{ $labels.kubernetes_namespace }}) has no node marked as master. VALUE = {{ $value }}\n"
        resolved: "Redis master (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisTooManyMasters
      expr: redis_instance_info{role="master"} > 1
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis too many masters (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n" 
        description: "Redis cluster on (namespace {{ $labels.kubernetes_namespace }}) has too many nodes marked as master. VALUE = {{ $value }}\n"
        resolved: "Redis many masters (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisDisconnectedSlaves
      expr: count without (instance, job) (redis_connected_slaves) - sum without (instance, job) (redis_connected_slaves) - 1 > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis disconnected slaves (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis not replicating for all slaves on (namespace {{ $labels.kubernetes_namespace }})  . Consider reviewing the redis replication status. VALUE = {{ $value }}\n"
        resolved: "Redis slaves (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisReplicationBroken
      expr: delta(redis_connected_slaves[1m]) < 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis replication broken (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"  
        description: "Redis instance lost a slave on (namespace {{ $labels.kubernetes_namespace }})  VALUE = {{ $value }}\n"
        resolved: "Redis replication (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisClusterFlapping
      expr: changes(redis_connected_slaves[1m]) > 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Redis cluster flapping (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"  
        description: "Changes have been detected in Redis replica connection on (namespace {{ $labels.kubernetes_namespace }}) . This can occur when replica nodes lose connection to the master and reconnect (a.k.a flapping). VALUE = {{ $value }}\n"
        resolved: "Redis cluster (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisOutOfSystemMemory
      expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 > 90
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Redis out of system memory (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis is running out of system memory (> 90%) VALUE = {{ $value }}\n"
        resolved: "Redis out of system memory (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"

    - alert: RedisTooManyConnections
      expr: redis_connected_clients / redis_config_maxclients * 100 > 90
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Redis too many connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis is running out of connections (> 90% used) VALUE = {{ $value }}\n"
        resolved: "Redis connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisNotEnoughConnections
      expr: redis_connected_clients < 4
      for: 2m
      labels:
        severity: warning 
      annotations:
        summary: "Redis not enough connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Redis instance should have more connections (> 4) VALUE = {{ $value }}\n"
        resolved: "Redis enough connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - alert: RedisRejectedConnections
      expr: increase(redis_rejected_connections_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Redis rejected connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
        description: "Some connections to Redis has been rejected on (namespace {{ $labels.kubernetes_namespace }}) VALUE = {{ $value }}\n"
        resolved: "Redis rejected connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
  - name: KubestateExporter
    rules:
    - alert: Deployment at 0 Replicas
      annotations:
        summary: "Deployment {{$labels.deployment}} is currently having no pods running\n"
        description: "Cluster Name: {{$externalLabels.cluster}}Namespace: {{$labels.namespace}}Deployment name: {{$labels.deployment}}\n" 
        resolved: "Deployment {{$labels.deployment}} is resolved"
      expr: |
        sum(kube_deployment_status_replicas{pod_template_hash=""}) by (deployment,namespace)  < 1
      for: 1m
      labels:
        severity: critical
    - alert: statefulset at 0 Replicas
      annotations:
        summary: "statefulset {{$labels.statefulset}} is currently having no pods running\n"
        description: "Cluster Name: {{$externalLabels.cluster}}Namespace: {{$labels.namespace}}statefulset name: {{$labels.statefulset}}\n" 
        resolved: "statefulset {{$labels.statefulset}} is resolved"
      expr: |
        sum(kube_statefulset_status_replicas{pod_template_hash=""}) by (statefulset,namespace)  < 1
      for: 1m
      labels:
        severity: critical
    - alert: KubernetesNodeunReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Node unready (instance {{ $labels.instance }})\n"
        description: "Node {{ $labels.node }} has been unready for a long time VALUE = {{ $value }}\n"
        resolved: "Kubernetes Node (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes memory pressure (instance {{ $labels.instance }})\n"
        description: "{{ $labels.node }} has MemoryPressure condition VALUE = {{ $value }}\n"
        resolved: "Kubernetes memory pressure (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes disk pressure (instance {{ $labels.instance }})\n"
        description: "{{ $labels.node }} has DiskPressure condition VALUE = {{ $value }}\n"
        resolved: "Kubernetes disk pressure (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesNetworkUnavailable
      expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes network unavailable (instance {{ $labels.instance }})\n"
        description: "{{ $labels.node }} has NetworkUnavailable condition VALUE = {{ $value }}\n"
        resolved: "Kubernetes network  (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesOutOfCapacity
      expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes out of capacity (instance {{ $labels.instance }})\n"
        description: "{{ $labels.node }} is out of capacity VALUE = {{ $value }}\n"
        resolved: "Kubernetes capacity  (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesContainerOomKiller
      expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes container oom killer (instance {{ $labels.instance }})\n"
        description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes. VALUE = {{ $value }}\n"
        resolved: "Kubernetes container  (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesJobFailed
      expr: kube_job_status_failed > 0
      for: 0m
      labels:
        severity: warning 
      annotations:
        summary: "Kubernetes Job failed (instance {{ $labels.instance }})\n"
        description: "Job {{ $labels.namespace }}/{{ $labels.exported_job }} failed to complete VALUE = {{ $value }}\n"
        resolved: "Kubernetes Job (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesCronjobSuspended
      expr: kube_cronjob_spec_suspend != 0
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})\n"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended VALUE = {{ $value }}\n"
        resolved: "Kubernetes CronJob (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesPersistentvolumeclaimPending
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending VALUE = {{ $value }}\n"
        resolved: "Kubernetes PersistentVolumeClaim (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesVolumeOutOfDiskSpace
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning 
      annotations:
        summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})\n"
        description: "Volume is almost full (< 10% left) VALUE = {{ $value }}\n"
        resolved: "Kubernetes disk space (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesPersistentvolumeError
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})\n"
        description: "Persistent volume is in bad state VALUE = {{ $value }}\n"
        resolved: "Kubernetes PersistentVolume (instance {{ $labels.instance }}) is resolved and available"
    - alert: KubernetesStatefulsetDown
      expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0
      for: 1m
      labels:
        severity: critical 
      annotations:
        summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})\n"
        description: "{{ $labels.statefulset }} on  {{ $labels.namespace }}  StatefulSet went down VALUE = {{ $value }}\n"
        resolved: "Kubernetes StatefulSet (instance {{ $labels.instance }}) is resolved and up"

    - alert: KubernetesPodNotHealthy
      expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
      for: 15m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})\n"
        description: "Pod {{ $labels.pod }} on {{ $labels.namespace }} has been in a non-ready state for longer than 15 minutes. VALUE = {{ $value }}\n"
        resolved: "Kubernetes Pod (instance {{ $labels.instance }}) is resolved and healthy"
    - alert: KubernetesPodCrashLooping
      expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})\n"
        description: "Pod {{ $labels.pod }} is crash looping VALUE = {{ $value }}\n"
        resolved: "Kubernetes Pod (instance {{ $labels.instance }}) is resolved and healthy"
    - alert: KubernetesReplicassetMismatch
      expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})  (replicaset {{ $labels.replicaset }}) (namespace {{ $labels.namespace }}) \n"
        description: "Deployment Replicas (replicaset {{ $labels.replicaset }}) (namespace {{ $labels.namespace }})  mismatch VALUE = {{ $value }}\n"
        resolved: "Kubernetes ReplicasSet  (instance {{ $labels.instance }})  (replicaset {{ $labels.replicaset }})  on (namespace {{ $labels.namespace }}) is resolved and healthy"
    - alert: KubernetesDeploymentReplicasMismatch
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})\n"
        description: "{{ $labels.deployment }} on  {{ $labels.namespace }}  Deployment Replicas mismatch VALUE = {{ $value }}\n"
        resolved: "Kubernetes Deployment replicas (instance {{ $labels.instance }})  is resolved"
    - alert: KubernetesStatefulsetReplicasMismatch
      expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})\n"
        description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet does not match the expected number of replicas. VALUE = {{ $value }}\n"
        resolved: "Kubernetes StatefulSet  replicas (instance {{ $labels.instance }})  is resolved"
    - alert: KubernetesDeploymentGenerationMismatch
      expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})\n"
        description: "{{ $labels.deployment }} on  {{ $labels.namespace }} Deployment has failed but has not been rolled back. VALUE = {{ $value }}\n"
        resolved: "Kubernetes Deployment generation (instance {{ $labels.instance }})  is resolved"
    - alert: KubernetesStatefulsetGenerationMismatch
      expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})\n"
        description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet has failed but has not been rolled back. VALUE = {{ $value }}\n"
        resolved: "Kubernetes StatefulSet generation (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesStatefulsetUpdateNotRolledOut
      expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})\n"
        description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet update has not been rolled out. VALUE = {{ $value }}\n"
        resolved: "Kubernetes StatefulSet (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesDaemonsetRolloutStuck
      expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})\n"
        description: "Some Pods of DaemonSet are not scheduled or not ready VALUE = {{ $value }}\n"
        resolved: "Kubernetes DaemonSet (instance {{ $labels.instance }}) is resolved"
    - alert: KubernetesDaemonsetMisscheduled
      expr: kube_daemonset_status_number_misscheduled > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})\n"
        description: "Some DaemonSet Pods are running where they are not supposed to run VALUE = {{ $value }}\n"
        resolved: "Kubernetes DaemonSet (instance {{ $labels.instance }}) is resolved"
  - name: Argocd_alerts
    rules:
    - alert: ArgocdServiceNotSynced
      expr: sum(argocd_app_info{sync_status!="Synced",name!="dartil-ing-disable-test",name!="dartil-ing-disable-develop",name!="dartil-ing-disable-stage"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type) #argocd_app_info{sync_status!="Synced",name!="dartil-ing-disable-test",name!="dartil-ing-disable-develop"} != 0
      for: 5m
      labels:
        severity: warning
        type: argocd-metrics
      annotations:
        summary: "ArgoCD service not synced (instance {{ $labels.name }})\n"
        description: "Service {{ $labels.name }} run by argo is currently not in sync. \n"
        resolved: "ArgoCD service {{ $labels.name }} is resolved"
    - alert: ArgocdServiceUnknown
      expr: sum( argocd_app_info{sync_status="Unknown"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type) #argocd_app_info{sync_status="Unknown"} != 0
      for: 5m
      labels:
        severity: warning
        type: argocd-metrics
      annotations:
        summary: "ArgoCD service Unknown (instance {{ $labels.name }})\n"
        description: "Service {{ $labels.name }} run by argo is currently in Unknown. \n"
        resolved: "ArgoCD service {{ $labels.name }} is resolved"

    - alert: ArgocdServiceUnhealthy
      expr: sum(argocd_app_info{health_status!="Healthy",health_status!="Progressing"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type)
      for: 5m
      labels:
        severity: warning
        type: argocd-metrics
      annotations:
        summary: "ArgoCD service unhealthy (instance {{ $labels.name }})\n"
        description: "Service {{ $labels.name }} run by argo is currently not healthy. \n"
        resolved: "ArgoCD service {{ $labels.name }} is resolved"
  - name: MSSQL_alerts
    rules:
    - alert: SqlServerDeadlock
      expr: increase(mssql_deadlocks[3m]) > 2
      for: 0m
      labels:
        severity: warning
        type: mssql-metrics
      annotations:
        summary: "SQL Server deadlock (instance {{ $labels.instance }})\n"
        description: "SQL Server is having some deadlock. VALUE = {{ $value }}\n"
        resolved: "SQL Server deadlock (instance {{ $labels.instance }}) is resolved"
  - name: BlackBox_alerts
    rules:
    - alert: BlackboxProbDuration
      expr: probe_duration_seconds{instance="https://tapsi.shop"} > 5
      for: 10s
      labels:
        severity: critical
        type: blackbox_metrics
      annotations:
        summary: "Blackbox slow probe (instance {{ $labels.instance }}) on  job {{ $labels.job }} \n"
        description: "Blackbox probe took more than 5s to complete\n  VALUE = {{ $value }}\n "
        resolved: "Blackbox slow probe (instance {{ $labels.instance }}) on  job {{ $labels.job }} is resolved"



    # - alert: BlackboxProbeHttpFailure
    #   expr: probe_http_status_code <= 199 OR probe_http_status_code >= 499
    #   for: 1m
    #   labels:
    #     severity: critical
    #     type: blackbox_metrics
    #   annotations:
    #     summary: "Blackbox probe HTTP failure (instance {{ $labels.instance }}) on job {{ $labels.job }} \n"
    #     description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n"
    #     resolved: "Blackbox probe HTTP (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"



    - alert: BlackboxSslCertificateWillExpireSoon
      expr: 3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20
      for: 0m
      labels:
        severity: warning
        type: blackbox_metrics
      annotations:
        summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }}) on job {{ $labels.job }}\n"
        description: "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}\n" 
        resolved: "Blackbox  SSL certificate (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"



    - alert: BlackboxProbeSlowPing
      expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 5
      for: 1m
      labels:
        severity: warning
        type: blackbox_metrics
      annotations:
        summary: "Blackbox probe slow ping (instance {{ $labels.instance }})\n"
        description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n"     
        resolved: "Blackbox probe ping (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
  - name: elasticsearch
    rules:
    - alert: ElasticsearchHeapUsageTooHigh
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
      for: 2m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Heap Usage Too High (name {{ $labels.name }})\n"
        description: "The heap usage is over 90%  VALUE = {{ $value }}\n"
        resolved: "Elasticsearch Heap Usage (name {{ $labels.name }}) is resolved"

    - alert: ElasticsearchHeapUsageWarning
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
      for: 2m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Heap Usage warning (name {{ $labels.name }})\n"
        description: "The heap usage is over 80% VALUE = {{ $value }}\n"
        resolved: "Elasticsearch Heap Usage (name {{ $labels.name }}) is resolved"

    - alert: ElasticsearchDiskOutOfSpace
      expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10
      for: 0m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch disk out of space (name {{ $labels.name }})\n"
        description: "The disk usage is over 90% VALUE = {{ $value }}\n"
        resolved: "Elasticsearch disk space (name {{ $labels.name }}) is resolved"

    - alert: ElasticsearchDiskSpaceLow
      expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20
      for: 2m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch disk space low (name {{ $labels.name }})\n"
        description: "The disk usage is over 80% VALUE = {{ $value }}\n"
        resolved: "Elasticsearch disk space (name {{ $labels.name }}) is resolved"

    - alert: ElasticsearchClusterRed
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 0m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Cluster Red (instance {{ $labels.instance }})\n"
        description: "Elastic Cluster Red status VALUE = {{ $value }}\n"
        resolved: "Elasticsearch Cluster (instance {{ $labels.instance }}) is resolved"

    - alert: ElasticsearchClusterYellow
      expr: elasticsearch_cluster_health_status{color="yellow"} == 1
      for: 0m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Cluster Yellow (instance {{ $labels.instance }})\n"
        description: "Elastic Cluster Yellow status VALUE = {{ $value }}\n"
        resolved: "Elasticsearch Cluster (instance {{ $labels.instance }}) is resolved"

    - alert: ElasticsearchHealthyNodes
      expr: elasticsearch_cluster_health_number_of_nodes < 3
      for: 0m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Healthy Nodes (instance {{ $labels.instance }})\n"
        description: "Missing node in Elasticsearch cluster VALUE = {{ $value }}\n"
        resolved: "Elasticsearch nodes (instance {{ $labels.instance }}) is resolved"

    - alert: ElasticsearchHealthyDataNodes
      expr: elasticsearch_cluster_health_number_of_data_nodes < 3
      for: 0m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch Healthy Data Nodes (instance {{ $labels.instance }})\n"
        description: "Missing data node in Elasticsearch cluster VALUE = {{ $value }}\n"
        resolved: "Elasticsearch data nodes (instance {{ $labels.instance }}) is resolved"

    - alert: ElasticsearchRelocatingShardsTooLong
      expr: elasticsearch_cluster_health_relocating_shards > 0
      for: 15m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch relocating shards too long (instance {{ $labels.instance }})\n"
        description: "Elasticsearch has been relocating shards for 15min VALUE = {{ $value }}\n"
        resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"

    - alert: ElasticsearchInitializingShards
      expr: elasticsearch_cluster_health_initializing_shards > 0
      for: 0m
      labels:
        severity: info
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch initializing shards (instance {{ $labels.instance }})\n"
        description: "Elasticsearch is initializing shards VALUE = {{ $value }}\n"
        resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"

    - alert: ElasticsearchInitializingShardsTooLong
      expr: elasticsearch_cluster_health_initializing_shards > 0
      for: 15m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch initializing shards too long (instance {{ $labels.instance }})\n"
        description: "Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }}\n"
        resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"

    - alert: ElasticsearchUnassignedShards
      expr: elasticsearch_cluster_health_unassigned_shards > 0
      for: 0m
      labels:
        severity: critical
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch unassigned shards (instance {{ $labels.instance }})\n"
        description: "Elasticsearch has unassigned shards VALUE = {{ $value }}\n"
        resolved: "Elasticsearch unassigned shards (instance {{ $labels.instance }}) is resolved"

    - alert: ElasticsearchPendingTasks
      expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
      for: 15m
      labels:
        severity: warning
        type: monitoring-metrics
      annotations:
        summary: "Elasticsearch pending tasks (instance {{ $labels.instance }})\n"
        description: "Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }}\n"
        resolved: "Elasticsearch pending tasks (instance {{ $labels.instance }}) is resolved"

  #  - name: Nginx_alerts
  #    rules:
  #    - alert: NginxHighHttp5xxErrorRate
  #      expr: sum(nginx_ingress_controller_requests{status!~"[5].*",ingress=~".*tapsi.shop"}) by (ingress) / sum(nginx_ingress_controller_requests{ingress=~".*tapsi.shop"}) by (ingress) * 100 < 97
  #      for: 2m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "Nginx high HTTP 5xx error rate (ingress {{ $labels.ingress }})\n"
  #        description: "Too many HTTP requests with status 5xx (> 5%)\n  VALUE = {{ $value }}\n"
  #        resolved: "Nginx high HTTP 5xx error for  (ingress {{ $labels.ingress }}) is resolved"

  #    - alert: HighLatency
  #      expr: histogram_quantile(0.95,sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{ingress=~".*tapsi.shop"}[15m])) by (le,ingress)) > 10
  #      for: 1m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "High Latency rate on (ingress {{ $labels.ingress }})\n"
  #        description: "High Latency rate  on (ingress {{ $labels.ingress }})\n  VALUE = {{ $value }}\n"
  #        resolved: "High Latency rate on (ingress {{ $labels.ingress }}) is resolved"

  #    - alert: HighResponseTime
  #      expr: histogram_quantile(0.95,sum(rate(nginx_ingress_controller_response_duration_seconds_bucket{ingress=~".*tapsi.shop"}[15m])) by (le,ingress)) > 1
  #      for: 1m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "High ResponseTime on (ingress {{ $labels.ingress }})\n"
  #        description: "High ResponseTime  on (ingress {{ $labels.ingress }})\n  VALUE = {{ $value }}\n"
  #        resolved: "High ResponseTime on (ingress {{ $labels.ingress }}) is resolved" 

  #    - alert: Highrequestrate
  #      expr: sum(rate(nginx_ingress_controller_requests{ingress=~".*tapsi.shop"}[5m])) by (ingress) > 800
  #      for: 1m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "High request rate on  (ingress {{ $labels.ingress }})\n"
  #        description: "High request rate on  (ingress {{ $labels.ingress }})\n  VALUE = {{ $value }}\n"       
  #        resolved: "High request rate on (ingress {{ $labels.ingress }}) is resolved"

  #    - alert: lowrequestrate
  #      expr: sum(rate(nginx_ingress_controller_requests{ingress=~".*tapsi.shop"}[5m])) by (ingress) > 10
  #      for: 1m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "low request rate on  (ingress {{ $labels.ingress }})\n"
  #        description: "low request rate on  (ingress {{ $labels.ingress }})\n  VALUE = {{ $value }}\n"       
  #        resolved: "low request rate on (ingress {{ $labels.ingress }}) is resolved" 

  #    - alert: Connectionrate
  #      expr: sum(rate(nginx_ingress_controller_nginx_process_connections{controller_class="k8s.io/ingress-nginx"}[5m])) > 100
  #      for: 1m
  #      labels:
  #        severity: critical
  #        type: "telegram-alerts"
  #      annotations:
  #        summary: "High Connection rate on (ingress {{ $labels.ingress }})\n"
  #        description: "High request rate on (ingress {{ $labels.ingress }})\n  VALUE = {{ $value }}\n"              
  #        resolved: "High Connection rate on (ingress {{ $labels.ingress }}) is resolved"  

  - name: Gateway_Alert
    rules:
    - alert: UnormalRequestRate
      expr: sum(http_client_active_requests) by (server_address) > 1000
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Request rate of server {{ $labels.server_address }}\n"
        description: "Request rate of server {{ $labels.server_address }} = {{ $value }} \n"
        resolved: "Unormal Request rate of server {{ $labels.server_address }} has been resolved\n"
  - name: Mongo_Alert
    rules:
    - alert: MongoDBstatus
      expr: mongodb_members_health{kubernetes_namespace="stage",rs_nm="rs0"} != 1
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Mongodb is not Healthy\n"
        description: "Mongodb {{ $labels.member_idx }} is not Healthy\n"
        resolved: "Mongodb {{ $labels.member_idx }} is Healthy\n"

    - alert: MongoDBReplicationLag
      expr: ((max(mongodb_rs_members_optimeDate{member_state="PRIMARY"}) - min(mongodb_rs_members_optimeDate{member_state="SECONDARY"})) / 1000 ) > 500
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Mongodb Replication Lag\n"
        description: "Mongodb has more than 500 ms lag for replication\n"
        resolved: "Mongodb replication lag resolved\n"
  - name: Redis_Alert
    rules:
    - alert: RedisStatus
      expr: redis_up{kubernetes_namespace="stage"} != 1
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Redis is not Healthy\n"
        description: "Redis {{ $labels.cluster }} is not Healthy\n"
        resolved: "Redis {{ $labels.cluster }} is Healthy\n"
  - name: RabbitMQ_Alert
    rules:
    - alert: RabbitMQstatus
      expr: up{kubernetes_namespace="stage",kubernetes_pod_name="rabbit-rabbitmq-0"} != 1
      for: 2m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "RabbitMQ is not Healthy\n"
        description: "RabbitMQ {{ $labels.cluster }} is not Healthy\n"
        resolved: "RabbitMQ {{ $labels.cluster }} is Healthy\n"
    
    - alert: RabbitMQunackedMessages
      expr: rabbitmq_queue_messages_unacked{} > 1000
      for: 5m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "More than 1000 Unacked Messages\n"
        description: "Unacked Messages = {{ $value }} \n"
        resolved: "Unacked Messages = {{ $value }} \n"
  - name: SqlServer_Alert
    rules:
    - alert: SqlServerStatus
      expr: windows_service_state{name=~"mssql\\$node|mssqlserver",state="running"} != 1
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "SqlServer is not Healthy\n"
        description: "SqlServer {{ $labels.environment }} {{ $labels.instance }} is not Healthy\n"
        resolved: "SqlServer {{ $labels.environment }} {{ $labels.instance }} is Healthy\n"
    - alert: SQL_Cluster_Service
      expr: windows_service_state{instance=~"172.31.58.*",name="clussvc",state="running"} != 1
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "SQL Cluster Service is Down\n"
        description: "SQL Cluster Service is Down on {{ $labels.environment }} {{ $labels.instance }} \n"
        resolved: "SQL Cluster Service is UP on {{ $labels.environment }} {{ $labels.instance }} \n"
  - name: Certificate_Alert
    rules:
    - alert: SslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry{instance=~"https://tapsi.shop|https://minio-admin.hasti.co|https://qcommercegw.dartil.com"} - time() < (3600 * 24 * 7)
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Ssl Certificate Will Expire Soon\n"
        description: "Ssl Certificate of {{ $labels.instance }} Will Expire after 7 days\n"
        resolved: "Ssl Certificate renewed\n"
  - name: DiskUsage
    rules:
    - alert: LinuxServersDiskUsage
      expr: sum by (instance,mountpoint) ((node_filesystem_size_bytes{}-node_filesystem_free_bytes{})/node_filesystem_size_bytes{}*100) > 85
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Disk usage is high\n"
        description: "Disk usage on {{ $labels.instance }} {{ $labels.mountpoint }} is = {{ $value }} \n"
        resolved: "Disk Usage {{ $labels.instance }} {{ $labels.mountpoint }} has been resolved\n"

    - alert: WindowsServersDiskUsage
      expr: sum by (environment, volume, instance) ((windows_logical_disk_size_bytes{volume !~ "HarddiskVolume.+", volume!="F:\\Data01"} - windows_logical_disk_free_bytes{volume !~ "HarddiskVolume.+", volume!="F:\\Data01"}) / windows_logical_disk_size_bytes{volume !~ "HarddiskVolume.+", volume!="F:\\Data01"} * 100) > 85
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Disk Usage is High\n"
        description: "Disk available on {{ $labels.environment }} {{ $labels.instance }} {{ $labels.volume }} Disk Usage is = {{ $value }} \n"
        resolved: "Disk Usage {{ $labels.environment }} {{ $labels.instance }} {{ $labels.volume }} has been resolved\n"
  - name: DNS-Resolve
    rules:
    - alert: DNS_Resolve_Problem
      expr: probe_success{job="blackbox-dns-arvan",instance="1.1.1.1"} != 1
      for: 1m
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "DNS Resolve Problem\n"
        description: "DNS of Tapsi.Shop not resolve to 194.156.140.212 \n"
        resolved: "DNS of Tapsi.Shop resolve to 194.156.140.212\n"
  - name: UnSync-Arvan-S3
    rules:
    - alert: UnSync_Contents
      expr: mc_mirror_failed_s3ops{} > 10
      for: 0m
      labels:
        severity: critical
        type: "telegram-alerts"
        type2: "minio"
      annotations:
        summary: "Sync with Arvan Problem\n"
        description: "We have {{ $value }} Unsync Contents \n"
        resolved: "Unsync has been resolved\n"
  - name: API-Targets-Unavailable
    rules:
    - alert: Internal_Target_Unavailable
      expr: probe_success{job="blackbox-http-arvan"} == 0
      for: 60s
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Internal Target is unavailable\n"
        description: "Internal Target is unavailable for >1m at {{ $labels.instance }}\n"
        resolved: "Internal Target API is available at {{ $labels.instance }}\n"
    - alert: External_Target_Unavailable
      expr: probe_success{job="blackbox-http-local"} == 0
      for: 120s
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "External Target API is unavailable\n"
        description: "External Target API is unavailable at {{ $labels.instance }}\n"
        resolved: "External Target API is available at {{ $labels.instance }}\n"
    - alert: Searchia_is_unavailable
      expr: probe_success{job="blackbox-searchia-search-local",instance="https://searchia\\.ir/api/index/product\\?optionalFilters=is_available%3Cscore%3D1%3E&from=0&filters=is_available%3Atrue&size=100&filters=vendorId%3A1042047037807263744&lat=35\\.6997384&lon=51\\.3380605&query=%D8%AA%DB%8C%D9%86%D8%AA%20%D9%84%D8%A8"} == 0
      for: 120s
      labels:
        severity: critical
        type: "telegram-alerts"
      annotations:
        summary: "Searchia API probe is unavailable\n"
        description: "Searchia API probe failed for 30s at {{ $labels.instance }}\n"
        resolved: "Searchia API probe is available at {{ $labels.instance }}\n"
