# yaml-language-server: $schema=values.schema.json
# Default values for prometheus.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

rbac:
  create: true

podSecurityPolicy:
  enabled: false

imagePullSecrets: []
# - name: "image-pull-secret"

## Define serviceAccount names for components. Defaults to component's fully qualified name.
##
serviceAccounts:
  server:
    create: true
    name: ""
    annotations: {}

    ## Opt out of automounting Kubernetes API credentials.
    ## It will be overriden by server.automountServiceAccountToken value, if set.
    # automountServiceAccountToken: false

## Additional labels to attach to all resources
commonMetaLabels: {}

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/prometheus-operator/prometheus-operator/tree/main/cmd/prometheus-config-reloader
##
configmapReload:
  ## URL for configmap-reload to use for reloads
  ##
  reloadUrl: ""

  ## env sets environment variables to pass to the container. Can be set as name/value pairs,
  ## read from secrets or configmaps.
  env: []
    # - name: SOMEVAR
    #   value: somevalue
    # - name: PASSWORD
    #   valueFrom:
    #     secretKeyRef:
    #       name: mysecret
    #       key: password
    #       optional: false

  prometheus:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true

    ## configmap-reload container name
    ##
    name: configmap-reload

    ## configmap-reload container image
    ##
    image:
      repository: docker-proxy.hasti.co/prometheus-operator/prometheus-config-reloader
      tag: v0.70.0
      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
      digest: ""
      pullPolicy: IfNotPresent

    # containerPort: 9533

    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}

    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []

    ## Additional configmap-reload volume mounts
    ##
    extraVolumeMounts: []

    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true

    ## Security context to be added to configmap-reload container
    containerSecurityContext: {}

    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}

server:
  ## Prometheus server container name
  ##
  name: server

  ## Opt out of automounting Kubernetes API credentials.
  ## If set it will override serviceAccounts.server.automountServiceAccountToken value for ServiceAccount.
  # automountServiceAccountToken: false

  ## Use a ClusterRole (and ClusterRoleBinding)
  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY
  ##
  ## NB: because we need a Role with nonResourceURL's ("/metrics") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.
  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.
  ##
  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.
  ##
  # useExistingClusterRoleName: nameofclusterrole

  ## If set it will override prometheus.server.fullname value for ClusterRole and ClusterRoleBinding
  ##
  clusterRoleNameOverride: ""

  # Enable only the release namespace for monitoring. By default all namespaces are monitored.
  # If releaseNamespace and namespaces are both set a merged list will be monitored.
  releaseNamespace: false

  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.
  # namespaces:
  #   - yournamespace

  # sidecarContainers - add more containers to prometheus server
  # Key/Value where Key is the sidecar `- name: <Key>`
  # Example:
  #   sidecarContainers:
  #      webserver:
  #        image: nginx
  # OR for adding OAuth authentication to Prometheus
  #   sidecarContainers:
  #     oauth-proxy:
  #       image: quay.io/oauth2-proxy/oauth2-proxy:v7.1.2
  #       args:
  #       - --upstream=http://127.0.0.1:9090
  #       - --http-address=0.0.0.0:8081
  #       - ...
  #       ports:
  #       - containerPort: 8081
  #         name: oauth-proxy
  #         protocol: TCP
  #       resources: {}
  sidecarContainers: {}

  # sidecarTemplateValues - context to be used in template for sidecarContainers
  # Example:
  #   sidecarTemplateValues: *your-custom-globals
  #   sidecarContainers:
  #     webserver: |-
  #       {{ include "webserver-container-template" . }}
  # Template for `webserver-container-template` might looks like this:
  #   image: "{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}"
  #   ...
  #
  sidecarTemplateValues: {}

  ## Prometheus server container image
  ##
  image:
    repository: docker-proxy.hasti.co/bitnami/prometheus
    # if not set appVersion field from Chart.yaml is used
    tag: "2.52.1-debian-12-r1" 
    #tag: "2.48.0-debian-11-r0"
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""
    pullPolicy: IfNotPresent

  ## Prometheus server command
  ##
  command: []

  ## prometheus server priorityClassName
  ##
  priorityClassName: ""

  ## EnableServiceLinks indicates whether information about services should be injected
  ## into pod's environment variables, matching the syntax of Docker links.
  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.
  ##
  enableServiceLinks: true

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  prefixURL: ""

  ## External URL which can access prometheus
  ## Maybe same with Ingress host name
  baseURL: ""

  ## Additional server container environment variables
  ##
  ## You specify this manually like you would a raw deployment manifest.
  ## This means you can bind in environment variables from secrets.
  ##
  ## e.g. static environment variable:
  ##  - name: DEMO_GREETING
  ##    value: "Hello from the environment"
  ##
  ## e.g. secret environment variable:
  ## - name: USERNAME
  ##   valueFrom:
  ##     secretKeyRef:
  ##       name: mysecret
  ##       key: username
  env: []

  # List of flags to override default parameters, e.g:
  # - --enable-feature=agent
  # - --storage.agent.retention.max-time=30m
  # - --config.file=/etc/config/prometheus.yml
  defaultFlagsOverride: []

  extraFlags:
    - web.enable-lifecycle
    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as
    ## deleting time series. This is disabled by default.
    # - web.enable-admin-api
    ##
    ## storage.tsdb.no-lockfile flag controls BD locking
    # - storage.tsdb.no-lockfile
    ##
    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)
    # - storage.tsdb.wal-compression

  ## Path to a configuration file on prometheus server container FS
  configPath: /etc/config/prometheus.yml

  ### The data directory used by prometheus to set --storage.tsdb.path
  ### When empty server.persistentVolume.mountPath is used instead
  storagePath: ""

  global:
    ## How frequently to scrape targets by default
    ##
    scrape_interval: 3m
    ## How long until a scrape request times out
    ##
    scrape_timeout: 30s
    ## How frequently to evaluate rules
    ##
    evaluation_interval: 3m
  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
  ##
  remoteWrite:
  - url: "http://thanos-receive:19291/api/v1/receive" ###"https://remote-write-k8s.hasti.co/api/v1/receive"
    headers:
      THANOS-TENANT: a
      cluster: stage

  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read
  ##
  remoteRead: []

  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
  ##
  tsdb: {}
    # out_of_order_time_window: 0s

  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#exemplars
  ## Must be enabled via --enable-feature=exemplar-storage
  ##
  exemplars: {}
    # max_exemplars: 100000

  ## Custom HTTP headers for Liveness/Readiness/Startup Probe
  ##
  ## Useful for providing HTTP Basic Auth to healthchecks
  probeHeaders: []
    # - name: "Authorization"
    #   value: "Bearer ABCDEabcde12345"

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ## Additional Prometheus server Volume mounts
  ##
  extraVolumeMounts: []

  ## Additional Prometheus server Volumes
  ##
  extraVolumes: []

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   subPath: ""
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /prometheus
    #   subPath: ""
    #   configMap: certs-configmap
    #   readOnly: true

  ## Additional Prometheus server Secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   subPath: ""
    #   secretName: prom-secret-files
    #   readOnly: true

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/server-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles)
  extraConfigmapLabels: {}

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    ## Prometheus server Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Prometheus server Ingress additional labels
    ##
    extraLabels: {}

    ## Redirect ingress to an additional defined port on the service
    # servicePort: 8081

    ## Prometheus server Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - prometheus.domain.com
    #   - domain.com/prometheus

    path: /

    # pathType is only for k8s >= 1.18
    pathType: Prefix

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server Deployment Strategy type
  strategy:
    type: Recreate

  ## hostAliases allows adding entries to /etc/hosts inside the containers
  hostAliases: []
  #   - ip: "127.0.0.1"
  #     hostnames:
  #       - "example.com"

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}

  ## Pod affinity
  ##
  affinity: {}

  ## Pod topology spread constraints
  ## ref. https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
  topologySpreadConstraints: []

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1
    # minAvailable: 1
    ## unhealthyPodEvictionPolicy is available since 1.27.0 (beta)
    ## https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy
    # unhealthyPodEvictionPolicy: IfHealthyBudget

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: false

    ## If set it will override the name of the created persistent volume claim
    ## generated by the stateful set.
    ##
    statefulSetNameOverride: ""

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume labels
    ##
    labels: {}

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 8Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Prometheus server data Persistent Volume Binding Mode
    ## If defined, volumeBindingMode: <volumeBindingMode>
    ## If undefined (the default) or set to null, no volumeBindingMode spec is
    ##   set, choosing the default mode.
    ##
    # volumeBindingMode: ""

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

    ## Persistent Volume Claim Selector
    ## Useful if Persistent Volumes have been provisioned in advance
    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector
    ##
    # selector:
    #  matchLabels:
    #    release: "stable"
    #  matchExpressions:
    #    - { key: environment, operator: In, values: [ dev ] }

    ## Persistent Volume Name
    ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one
    ##
    # volumeName: ""

  emptyDir:
    ## Prometheus server emptyDir volume size limit
    ##
    sizeLimit: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: prometheus

  ## Labels to be added to Prometheus server pods
  ##
  podLabels: {}

  ## Prometheus AlertManager configuration
  ##
  alertmanagers: []

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
  ##
  replicaCount: 1

  ## Number of old history to retain to allow rollback
  ## Default Kubernetes value is set to 10
  ##
  revisionHistoryLimit: 10

  ## Annotations to be added to deployment
  ##
  deploymentAnnotations: {}

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    ##
    enabled: false

    annotations: {}
    labels: {}
    podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    ##
    headless:
      annotations: {}
      labels: {}
      servicePort: 80
      ## Enable gRPC port on service to allow auto discovery with thanos-querier
      gRPC:
        enabled: false
        servicePort: 10901
        # nodePort: 10901

    ## Statefulset's persistent volume claim retention policy
    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether
    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down
    ## and deleting statefulset, respectively. Requires 1.27.0+.
    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
    ##
    pvcDeleteOnStsDelete: false
    pvcDeleteOnStsScale: false

  ## Prometheus server readiness and liveness probe initial delay and timeout
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  ##
  tcpSocketProbeEnabled: false
  probeScheme: HTTP
  readinessProbeInitialDelay: 30
  readinessProbePeriodSeconds: 5
  readinessProbeTimeout: 4
  readinessProbeFailureThreshold: 3
  readinessProbeSuccessThreshold: 1
  livenessProbeInitialDelay: 30
  livenessProbePeriodSeconds: 15
  livenessProbeTimeout: 10
  livenessProbeFailureThreshold: 3
  livenessProbeSuccessThreshold: 1
  startupProbe:
    enabled: false
    periodSeconds: 5
    failureThreshold: 30
    timeoutSeconds: 10

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 2000m
      memory: 4096Mi
    requests:
      cpu: 1000m
      memory: 2048Mi

  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
  ##
  hostNetwork: false

  # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically
  dnsPolicy: ClusterFirst

  # Use hostPort
  # hostPort: 9090

  # Use portName
  portName: ""

  ## Vertical Pod Autoscaler config
  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)
    enabled: false
#     updateMode: "Auto"
#     containerPolicies:
#     - containerName: 'prometheus-server'

  # Custom DNS configuration to be added to prometheus server pods
  dnsConfig: {}
    # nameservers:
    #   - 1.2.3.4
    # searches:
    #   - ns1.svc.cluster-domain.example
    #   - my.dns.search.suffix
    # options:
    #   - name: ndots
    #     value: "2"
  #   - name: edns0

  ## Security context to be added to server pods
  ##
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534

  ## Security context to be added to server container
  ##
  containerSecurityContext: {}

  service:
    ## If false, no Service will be created for the Prometheus server
    ##
    enabled: true

    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    sessionAffinity: None
    type: ClusterIP

    ## Enable gRPC port on service to allow auto discovery with thanos-querier
    gRPC:
      enabled: false
      servicePort: 10901
      # nodePort: 10901

    ## If using a statefulSet (statefulSet.enabled=true), configure the
    ## service to connect to a specific replica to have a consistent view
    ## of the data.
    statefulsetReplica:
      enabled: false
      replica: 0

    ## Additional port to define in the Service
    additionalPorts: []
    # additionalPorts:
    # - name: authenticated
    #   port: 8081
    #   targetPort: 8081

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (default if not specified is 15 days)
  ##
  retention: "1d"

  ## Prometheus' data retention size. Supported units: B, KB, MB, GB, TB, PB, EB.
  ##
  retentionSize: ""

## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)
ruleFiles: {}

## Prometheus server ConfigMap entries for scrape_config_files
## (allows scrape configs defined in additional files)
##
scrapeConfigFiles: []

## Prometheus server ConfigMap entries
##
serverFiles:
  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerting_rules.yml: 
   groups:
    - name: Container
      rules:
      - alert: Container restarted
        annotations:
          summary: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} was restarted\n"
          description: "Namespace: {{$labels.namespace}}  Pod name: {{$labels.pod}}  Container name: {{$labels.container}}\n" 
          resolved: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
        expr: |
          sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[1m])) by (pod,namespace,container) > 0
        for: 0m
        labels:
          severity: critical
      - alert: High Memory Usage of Container 
        annotations: 
          summary: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of Memory Limit.\n"
          description: "Cluster Name:  {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} Pod name: {{$labels.pod}} Container name: {{$labels.container}}\n" 
          resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
        expr: |
          ((( sum(container_memory_working_set_bytes{image!="",container!="POD", namespace!="kube-system"}) by (namespace,container,pod)  / sum(container_spec_memory_limit_bytes{image!="",container!="POD",namespace!="kube-system"}) by (namespace,container,pod) ) * 100 ) < +Inf ) > 75
        for: 5m
        labels: 
          severity: critical
      - alert: High CPU Usage of Container 
        annotations: 
          summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is using more than 75% of CPU Limit
          description: "Cluster Name:  {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} Pod name: {{$labels.pod}} Container name: {{$labels.container}}" 
          resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
        expr: |
          ((sum(irate(container_cpu_usage_seconds_total{image!="",container!="POD", namespace!="kube-system"}[30s])) by (namespace,container,pod) / sum(container_spec_cpu_quota{image!="",container!="POD", namespace!="kube-system"} / container_spec_cpu_period{image!="",container!="POD", namespace!="kube-system"}) by (namespace,container,pod) ) * 100)  > 75
        for: 5m
        labels: 
          severity: critical
      - alert: High Persistent Volume Usage
        annotations:
          summary: "Persistent Volume named {{$labels.persistentvolumeclaim}} in {{$labels.namespace}} is using more than 75% used. \n"
          description: "Cluster Name: {{$externalLabels.cluster}} Namespace: {{$labels.namespace}} PVC name: {{$labels.persistentvolumeclaim}}\n"
          resolved: "Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} is resolved"
        expr: |
          ((((sum(kubelet_volume_stats_used_bytes{})  by (namespace,persistentvolumeclaim))  / (sum(kubelet_volume_stats_capacity_bytes{}) by (namespace,persistentvolumeclaim)))*100) < +Inf ) > 75
        for: 5m
        labels:
          severity: critical
    - name: Nodes
      rules:
      - alert: High Node Memory Usage
        annotations:
          summary: "Node {{$labels.kubernetes_io_hostname}} has more than 80% memory used. Plan Capcity\n"
          description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
          resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
        expr: |
          (sum (container_memory_working_set_bytes{id="/",container!="POD"}) by (kubernetes_io_hostname) / sum (machine_memory_bytes{}) by (kubernetes_io_hostname) * 100) > 80
        for: 5m
        labels:
          severity: critical
          type: os-metrics
      - alert: High Node CPU Usage
        annotations:
          summary: "Node {{$labels.kubernetes_io_hostname}} has more than 80% allocatable cpu used. Plan Capacity.\n"
          description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
          resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
        expr: |
          (sum(rate(container_cpu_usage_seconds_total{id="/", container!="POD"}[1m])) by (kubernetes_io_hostname) / sum(machine_cpu_cores) by (kubernetes_io_hostname)  * 100) > 80
        for: 5m
        labels:
          severity: critical
          type: os-metrics
      - alert: High Node Disk Usage
        annotations:
          summary: "Node {{$labels.kubernetes_io_hostname}} has more than 85% disk used. Plan Capacity.\n"
          description: "Cluster Name: {{$externalLabels.cluster}} ode: {{$labels.kubernetes_io_hostname}}\n" 
          resolved: "Node {{$labels.kubernetes_io_hostname}} is resolved"
        expr: |
          (sum(container_fs_usage_bytes{device=~"^/dev/[sv]d[a-z][1-9]$",id="/",container!="POD"}) by (kubernetes_io_hostname) / sum(container_fs_limit_bytes{container!="POD",device=~"^/dev/[sv]d[a-z][1-9]$",id="/"}) by (kubernetes_io_hostname)) * 100 > 85
        for: 5m
        labels:
          severity: critical
          type: os-metrics
    - name: windows
      rules:
      - alert: WindowsServerCpuUsage
        expr: 100 - (avg by (instance) (rate(windows_cpu_time_total{mode="idle"}[2m])) * 100) > 85
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Windows Server CPU Usage (instance {{ $labels.instance }})\n"
          description: "CPU Usage is more than 85% VALUE = {{ $value }}\n"
          resolved: "Windows Server CPU Usage (instance {{ $labels.instance }}) is resolved"
      - alert: WindowsServerMemoryUsage
        expr: 100 - ((windows_os_physical_memory_free_bytes / windows_cs_physical_memory_bytes) * 100) > 90
        for: 2m
        labels:
          severity: critical
          type: os-metrics
        annotations:
          summary: "Windows Server memory Usage (instance {{ $labels.instance }})\n"
          description: "Memory usage is more than 90% VALUE = {{ $value }}\n"
          resolved: "Windows Server memory Usage (instance {{ $labels.instance }}) is resolved"
      - alert: WindowsServerDiskSpaceUsage
        expr: 100.0 - 100 * ((windows_logical_disk_free_bytes{volume !~ "HarddiskVolume.*"} / 1024 / 1024 ) / (windows_logical_disk_size_bytes{volume !~ "HarddiskVolume.*"} / 1024 / 1024)) > 85
        for: 2m
        labels:
          severity: critical
          type: os-metrics
        annotations:
          summary: "Windows Server disk Space Usage (instance {{ $labels.instance }})\n"
          description: "Disk usage is more than 85% VALUE = {{ $value }}\n"
          resolved: "Windows Server disk Space Usage (instance {{ $labels.instance }}) is resolved"
      #  - alert: WindowsServerServiceStatus
      #    expr: windows_service_status{status="ok"} != 1
      #    for: 1m
      #    labels:
      #      severity: critical
          
      #      type: os-metrics
      #    annotations:
      #      summary: "Windows Server service Status (instance {{ $labels.instance }})\n"
      #      description: "Windows Service state is not OK VALUE = {{ $value }}\n"
      #  - alert: WindowsServerCollectorError
      #    expr: windows_exporter_collector_success == 0
      #    for: 0m
      #    labels:
      #      severity: critical
      #      type: os-metrics
      #    annotations:
      #      summary: "Windows Server collector Error (instance {{ $labels.instance }})\n"
      #      description: "Collector {{ $labels.collector }} was not successful VALUE = {{ $value }}\n"
    - name: node_exporter_alerts
      rules:
      - alert: Windows server Node down
        expr: up{job="windows-servers"} == 0
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Node {{ $labels.instance }} is down\n"
          description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }} for more than 2 minutes. Node seems down.\n"
          resolved: "Node {{ $labels.instance }} is resolved and Up Now"
      - alert: Kubernetes Node down
        expr: sum(up{job="kubernetes-nodes-cadvisor"} == 0) by (environment,instance,job)
        for: 2m
        labels:
          severity: warning      
          type: os-metrics
        annotations:
          summary: "Node {{ $labels.instance }} is down\n"
          description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }}   {{ $labels.environment }} for more than 2 minutes. Node seems down.\n"   
          resolved: "Node {{ $labels.instance }} is resolved and Up Now"
      - alert: Node down
        expr: up{job="linux-servers"} == 0
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Node {{ $labels.instance }} is down\n"
          description: "Failed to scrape {{ $labels.job }} on {{ $labels.instance }} for more than 2 minutes. Node seems down.\n"
          resolved: "Node {{ $labels.instance }} is resolved and Up Now"
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host out of memory (instance {{ $labels.instance }})\n"
          description: "Node memory is filling up (< 10% left) VALUE = {{ $value }}\n"
          resolved: "Host out of memory (instance {{ $labels.instance }}) is resolved"
      - alert: HostMemoryUnderMemoryPressure
        expr: rate(node_vmstat_pgmajfault[1m]) > 2000
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host under memory pressure (instance {{ $labels.instance }})\n"
          description: "The node is under heavy memory pressure. High rate of major page faults VALUE = {{ $value }}\n"
          resolved: "Host under memory pressure (instance {{ $labels.instance }}) is resolved"
      # - alert: HostUnusualNetworkThroughputIn
      #   expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
      #   for: 5m
      #   labels:
      #     severity: warning
      #     type: os-metrics
      #   annotations:
      #     summary: "Host unusual network throughput in (instance {{ $labels.instance }})\n"
      #     description: "Host network interfaces are probably receiving too much data (> 100 MB/s) VALUE = {{ $value }}\n"
      # - alert: HostUnusualNetworkThroughputOut
      #   expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
      #   for: 5m
      #   labels:
      #     severity: warning
      #     type: os-metrics
      #   annotations:
      #     summary: "Host unusual network throughput out (instance {{ $labels.instance }})\n"
      #     description: "Host network interfaces are probably sending too much data (> 100 MB/s) VALUE = {{ $value }}\n"
      #  - alert: HostUnusualDiskReadRate
      #    expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 60
      #    for: 5m
      #    labels:
      #      severity: warning
      #      type: os-metrics
      #    annotations:
      #      summary: "Host unusual disk read rate (instance {{ $labels.instance }})\n"
      #      description: "Disk is probably reading too much data (> 60 MB/s) VALUE = {{ $value }}\n"
      # - alert: HostUnusualDiskWriteRate
      #   expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 80
      #   for: 2m
      #   labels:
      #     severity: warning
      #     type: os-metrics
      #   annotations:
      #     summary: "Host unusual disk write rate (instance {{ $labels.instance }})\n"
      #     description: "Disk is probably writing too much data (> 80 MB/s) VALUE = {{ $value }}\n"
      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host out of disk space (instance {{ $labels.instance }})\n"
          description: "Disk is almost full (< 10% left) VALUE = {{ $value }}\n"
          resolved: "Host out of disk space (instance {{ $labels.instance }}) is resolved"
      # - alert: HostDiskWillFillIn24Hours
      #   expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      #   for: 2m
      #   labels:
      #     severity: warning
      #     type: os-metrics
      #   annotations:
      #     summary: "Host disk will fill in 24 hours (instance {{ $labels.instance }})\n"
      #     description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate VALUE = {{ $value }}\n"
      - alert: HostOutOfInodes
        expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host out of inodes (instance {{ $labels.instance }})\n"
          description: "Disk is almost running out of available inodes (< 10% left) VALUE = {{ $value }}\n"
          resolved: "Host out of inodes (instance {{ $labels.instance }}) is resolved"
      # - alert: HostInodesWillFillIn24Hours
      #   expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
      #   for: 2m
      #   labels:
      #     severity: warning
      #     type: os-metrics
      #   annotations:
      #     summary: "Host inodes will fill in 24 hours (instance {{ $labels.instance }})\n"
      #     description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate VALUE = {{ $value }}\n"
      #  - alert: HostUnusualDiskReadLatency
      #    expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
      #    for: 2m
      #    labels:
      #      severity: warning
      #      type: os-metrics
      #    annotations:
      #      summary: "Host unusual disk read latency (instance {{ $labels.instance }})\n"
      #      description: "Disk latency is growing (read operations > 100ms) VALUE = {{ $value }}\n"
      #  - alert: HostUnusualDiskWriteLatency
      #    expr: rate(node_disk_write_time_seconds_totali{device!~"mmcblk.+"}[1m]) / rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0.1 and rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0
      #    for: 2m
      #    labels:
      #      severity: warning
      #      type: os-metrics
      #    annotations:
      #      summary: "Host unusual disk write latency (instance {{ $labels.instance }})\n"
      #      description: "Disk latency is growing (write operations > 100ms) VALUE = {{ $value }}\n"
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host high CPU load (instance {{ $labels.instance }})\n"
          description: "CPU load is > 80% VALUE = {{ $value }}\n"
          resolved: "Host CPU load (instance {{ $labels.instance }}) is resolved"
      - alert: HostCpuStealNoisyNeighbor
        expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host CPU steal noisy neighbor (instance {{ $labels.instance }})\n"
          description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit. VALUE = {{ $value }}\n"
          resolved: "Host CPU steal noisy neighbor (instance {{ $labels.instance }}) is resolved"
      - alert: HostContextSwitching
        expr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode="idle"})) > 20000
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host context switching (instance {{ $labels.instance }})\n"
          description: "Context switching is growing on node (> 20000 / s) VALUE = {{ $value }}\n"
          resolved: "Host context switching (instance {{ $labels.instance }}) is resolved"
      - alert: HostSwapIsFillingUp
        expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host swap is filling up (instance {{ $labels.instance }})\n"
          description: "Swap is filling up (>80%) VALUE = {{ $value }}\n"
          resolved: "Host swap  (instance {{ $labels.instance }}) is resolved"
      - alert: HostSystemdServiceCrashed
        expr: node_systemd_unit_state{state="failed"} == 1
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host SystemD service crashed (instance {{ $labels.instance }})\n"
          description: "SystemD service crashed VALUE = {{ $value }}\n"
          resolved: "Host SystemD service  (instance {{ $labels.instance }}) is resolved"
      - alert: HostPhysicalComponentTooHot
        expr: node_hwmon_temp_celsius > 75
        for: 5m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host physical component too hot (instance {{ $labels.instance }})\n"
          description: "Physical hardware component too hot VALUE = {{ $value }}\n"
          resolved: "Host physical component (instance {{ $labels.instance }}) is resolved"
      - alert: HostNodeOvertemperatureAlarm
        expr: node_hwmon_temp_crit_alarm_celsius == 1
        for: 0m
        labels:
          severity: critical
          type: os-metrics
        annotations:
          summary: "Host node overtemperature alarm (instance {{ $labels.instance }})\n"
          description: "Physical node temperature alarm triggered VALUE = {{ $value }}\n"
          resolved: "Host node overtemperature alarm (instance {{ $labels.instance }}) is resolved"
      - alert: HostRaidArrayGotInactive
        expr: node_md_state{state="inactive"} > 0
        for: 0m
        labels:
          severity: critical
          type: os-metrics
        annotations:
          summary: "Host RAID array got inactive (instance {{ $labels.instance }})\n"
          description: "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. VALUE = {{ $value }}\n"
          resolved: "Host RAID  (instance {{ $labels.instance }}) is resolved"
      - alert: HostRaidDiskFailure
        expr: node_md_disks{state="failed"} > 0
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host RAID disk failure (instance {{ $labels.instance }})\n"
          description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap VALUE = {{ $value }}\n"
          resolved: "Host RAID disk  (instance {{ $labels.instance }}) is resolved"
      - alert: HostOomKillDetected
        expr: increase(node_vmstat_oom_kill[1m]) > 0
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host OOM kill detected (instance {{ $labels.instance }})\n"
          description: "OOM kill detected VALUE = {{ $value }}\n"
          resolved: "Host  (instance {{ $labels.instance }}) is resolved"
      - alert: HostEdacCorrectableErrorsDetected
        expr: increase(node_edac_correctable_errors_total[1m]) > 0
        for: 0m
        labels:
          severity: info
          type: os-metrics
        annotations:
          summary: "Host EDAC Correctable Errors detected (instance {{ $labels.instance }})\n"
          description: Instance has had {{ printf "%.0f" $value }} correctable memory errors reported by EDAC in the last 5 minutes. VALUE = {{ $value }}\n
          resolved: "Host  (instance {{ $labels.instance }}) is resolved"
      - alert: HostEdacUncorrectableErrorsDetected
        expr: node_edac_uncorrectable_errors_total > 0
        for: 0m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})\n"
          description: Instance has had {{ printf "%.0f" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes. VALUE = {{ $value }}\n
          resolved: "Host  (instance {{ $labels.instance }}) is resolved"
      - alert: HostNetworkReceiveErrors
        expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host Network Receive Errors (instance {{ $labels.instance }}:{{ $labels.device }})\n"
          description: Instance interface has encountered {{ printf "%.0f" $value }} receive errors in the last five minutes. VALUE = {{ $value }}\n
          resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
      - alert: HostNetworkTransmitErrors
        expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
        for: 2m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host Network Transmit Errors (instance {{ $labels.instance }}:{{ $labels.device }})\n"
          description: Instance has encountered {{ printf "%.0f" $value }} transmit errors in the last five minutes. VALUE = {{ $value }}\n
          resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
      - alert: HostNetworkInterfaceSaturated
        expr: (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8
        for: 1m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host Network Interface Saturated (instance {{ $labels.instance }}:{{ $labels.interface }})\n"
          description: "The network interface is getting overloaded. VALUE = {{ $value }}\n"
      - alert: HostConntrackLimit
        expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8
        for: 5m
        labels:
          severity: warning
          type: os-metrics
        annotations:
          summary: "Host conntrack limit (instance {{ $labels.instance }})\n"
          description: "The number of conntrack is approching limit VALUE = {{ $value }}\n"
          resolved: "Host Network (instance {{ $labels.instance }}) is resolved"
      #  - alert: HostClockSkew
      #    expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)
      #    for: 2m
      #    labels:
      #      severity: warning
      #      type: os-metrics
      #    annotations:
      #      summary: "Host clock skew (instance {{ $labels.instance }})\n"
      #      description: "Clock skew detected. Clock is out of sync. VALUE = {{ $value }}\n"
      #  - alert: HostClockNotSynchronising
      #    expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds > 16
      #    for: 2m
      #    labels:
      #      severity: warning
      #      type: os-metrics
      #    annotations:
      #      summary: "Host clock not synchronising (instance {{ $labels.instance }})\n"
      #      description: "Clock not synchronising. VALUE = {{ $value }}\n"
    - name: MySql_alerts
      rules:
      - alert: MysqlDown
        expr: mysql_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MySQL down (instance {{ $labels.instance }})\n"
          description: "MySQL instance is down on {{ $labels.instance }} VALUE = {{ $value }}\n"
          resolved: "MySQL (instance {{ $labels.instance }}) is resolved and up"
      - alert: MysqlTooManyConnections(>80%)
        expr: max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL too many connections (> 80%) (instance {{ $labels.instance }})\n"
          description: "More than 80% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }}\n"
          resolved: "MySQL connections (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlHighThreadsRunning
        expr: max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections * 100 > 60
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL high threads running (instance {{ $labels.instance }})\n"
          description: "More than 60% of MySQL connections are in running state on {{ $labels.instance }} VALUE = {{ $value }}\n"
          resolved: "MySQL threads  (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlSlaveIoThreadNotRunning
        expr: ( mysql_slave_status_slave_io_running and ON (instance) mysql_slave_status_master_server_id > 0 ) == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MySQL Slave IO thread not running (instance {{ $labels.instance }})\n"
          description: "MySQL Slave IO thread not running on {{ $labels.instance }} VALUE = {{ $value }}\n"
          resolved: "MySQL Slave IO thread  (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlSlaveSqlThreadNotRunning
        expr: ( mysql_slave_status_slave_sql_running and ON (instance) mysql_slave_status_master_server_id > 0) == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MySQL Slave SQL thread not running (instance {{ $labels.instance }})\n"
          description: "MySQL Slave SQL thread not running on {{ $labels.instance }} VALUE = {{ $value }}\n"
          resolved: "MySQL Slave SQL thread  (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlSlaveReplicationLag
        expr: ( (mysql_slave_status_seconds_behind_master - mysql_slave_status_sql_delay) and ON (instance) mysql_slave_status_master_server_id > 0 ) > 30
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MySQL Slave replication lag (instance {{ $labels.instance }})\n"
          description: "MySQL replication lag on {{ $labels.instance }} VALUE = {{ $value }}\n" 
          resolved: "MySQL Slave replication lag (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlSlowQueries
        expr: increase(mysql_global_status_slow_queries[1m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL slow queries (instance {{ $labels.instance }})\n"
          description: "MySQL server mysql has some new slow query. VALUE = {{ $value }}\n"
          resolved: "MySQL slow queries (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlInnodbLogWaits
        expr: rate(mysql_global_status_innodb_log_waits[15m]) > 10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "MySQL InnoDB log waits (instance {{ $labels.instance }})\n"
          description: "MySQL innodb log writes stalling VALUE = {{ $value }}\n"
          resolved: "MySQL InnoDB log waits (instance {{ $labels.instance }}) is resolved"
      - alert: MysqlRestarted
        expr: mysql_global_status_uptime < 60
        for: 0m
        labels:
          severity: info
        annotations:
          summary: "MySQL restarted (instance {{ $labels.instance }})\n"
          description: "MySQL has just been restarted, less than one minute ago on {{ $labels.instance }}. VALUE = {{ $value }}\n"
          resolved: "MySQL  (instance {{ $labels.instance }}) is resolved"
    - name: Minio_alerts
      rules:
      - alert: MinioClusterDiskOffline
        expr: minio_cluster_disk_offline_total > 0
        for: 0m
        labels:
          severity: critical
          type:  minio
        annotations:
          summary: "Minio cluster disk offline (instance {{ $labels.instance }})\n"
          description: "Minio cluster disk is offline VALUE = {{ $value }}\n"
          resolved: "Minio cluster disk offline (instance {{ $labels.instance }}) is resolved"
      - alert: MinioNodeDiskOffline
        expr: minio_cluster_nodes_offline_total > 0
        for: 0m
        labels:
          severity: critical
          type:  minio
        annotations:
          summary: "Minio node disk offline (instance {{ $labels.instance }})\n"
          description: "Minio cluster node disk is offline VALUE = {{ $value }}\n"
          resolved: "Minio node disk offline (instance {{ $labels.instance }}) is resolved"
      - alert: MinioDiskSpaceUsage
        expr: disk_storage_available / disk_storage_total * 100 < 10
        for: 0m
        labels:
          severity: warning
          type:  minio
        annotations:
          summary: "Minio disk space usage (instance {{ $labels.instance }})\n"
          description: "Minio available free space is low (< 10%) VALUE = {{ $value }}\n"
          resolved: "Minio disk space usage (instance {{ $labels.instance }}) is resolved"
      - alert: MinioClient
        expr: mc_mirror_failed_s3ops{} > 10
        for: 5m
        labels:
          severity: critical
          type:  minio
        annotations:
          summary: "Minio client sync Failed for VALUE = {{ $value }}\n"
          description: "Some photos have not been sent to Arvan  VALUE = {{ $value }}\n"       
          resolved: "Minio client is synced"
    - name: Mongodb_alerts
      rules:
      - alert: MongodbDown
        expr: mongodb_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MongoDB Down (instance {{ $labels.instance }})\n"
          description: "MongoDB instance is down VALUE = {{ $value }}\n"
          resolved: "MongoDB (instance {{ $labels.instance }}) is resolved and up"
    - name: RabbitMq_alerts
      rules:
      - alert: RabbitmqMemoryHigh
        expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ memory high (instance {{ $labels.instance }})\n"
          description: "A node use more than 90% of allocated RAM VALUE = {{ $value }}\n"
          resolved: "RabbitMQ memory high (instance {{ $labels.instance }}) is resolved"
      - alert: RabbitmqFileDescriptorsUsage
        expr: rabbitmq_process_open_fds / rabbitmq_process_max_fds * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ file descriptors usage (instance {{ $labels.instance }})\n"
          description: "A node use more than 90% of file descriptors VALUE = {{ $value }}\n"
          resolved: "RabbitMQ file descriptors usage (instance {{ $labels.instance }}) is resolved"
      - alert: RabbitmqTooManyUnackMessages
        expr: sum(rabbitmq_queue_messages_unacked) BY (queue) > 1000
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ too many unack messages (instance {{ $labels.instance }})\n"
          description: "Too many unacknowledged messages VALUE = {{ $value }}\n"
          resolved: "RabbitMQ unack messages (instance {{ $labels.instance }}) is resolved"
      - alert: RabbitmqTooManyConnections
        expr: rabbitmq_connections > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ too many connections (instance {{ $labels.instance }})\n"
          description: "The total connections of a node is too high VALUE = {{ $value }}\n"
          resolved: "RabbitMQ connections (instance {{ $labels.instance }}) is resolved"
      - alert: RabbitmqNoQueueConsumer
        expr: rabbitmq_queue_consumers < 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ no queue consumer (instance {{ $labels.instance }})\n"
          description: "A queue has less than 1 consumer VALUE = {{ $value }}\n"
          resolved: "RabbitMQ queue consumer (instance {{ $labels.instance }}) is resolved"
      #  - alert: RabbitmqUnroutableMessages
      #    expr: increase(rabbitmq_channel_messages_unroutable_returned_total[1m]) > 0 or increase(rabbitmq_channel_messages_unroutable_dropped_total[1m]) > 0
      #    for: 2m
      #    labels:
      #      severity: warning
      #    annotations:
      #      summary: "RabbitMQ unroutable messages (instance {{ $labels.instance }})\n"
      #      description: "A queue has unroutable messages VALUE = {{ $value }}\n"
    - name: Redis_alerts
      rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis down (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis instance is on (namespace {{ $labels.kubernetes_namespace }}) down VALUE = {{ $value }}\n"
          resolved: "Redis (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved and up"
      - alert: RedisMissingMaster
        expr: (redis_instance_info{role="master"})  < 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis missing master (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis cluster on (namespace {{ $labels.kubernetes_namespace }}) has no node marked as master. VALUE = {{ $value }}\n"
          resolved: "Redis master (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisTooManyMasters
        expr: redis_instance_info{role="master"} > 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis too many masters (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n" 
          description: "Redis cluster on (namespace {{ $labels.kubernetes_namespace }}) has too many nodes marked as master. VALUE = {{ $value }}\n"
          resolved: "Redis many masters (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisDisconnectedSlaves
        expr: count without (instance, job) (redis_connected_slaves) - sum without (instance, job) (redis_connected_slaves) - 1 > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis disconnected slaves (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis not replicating for all slaves on (namespace {{ $labels.kubernetes_namespace }})  . Consider reviewing the redis replication status. VALUE = {{ $value }}\n"
          resolved: "Redis slaves (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisReplicationBroken
        expr: delta(redis_connected_slaves[1m]) < 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis replication broken (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"  
          description: "Redis instance lost a slave on (namespace {{ $labels.kubernetes_namespace }})  VALUE = {{ $value }}\n"
          resolved: "Redis replication (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisClusterFlapping
        expr: changes(redis_connected_slaves[1m]) > 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis cluster flapping (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"  
          description: "Changes have been detected in Redis replica connection on (namespace {{ $labels.kubernetes_namespace }}) . This can occur when replica nodes lose connection to the master and reconnect (a.k.a flapping). VALUE = {{ $value }}\n"
          resolved: "Redis cluster (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisOutOfSystemMemory
        expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Redis out of system memory (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis is running out of system memory (> 90%) VALUE = {{ $value }}\n"
          resolved: "Redis out of system memory (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      # - alert: RedisOutOfConfiguredMaxmemory
      #   expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
      #   for: 2m
      #   labels:
      #     severity: warning
      #   annotations:
      #    summary: "Redis out of configured maxmemory (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
      #     description: "Redis is running out of configured maxmemory on (namespace {{ $labels.kubernetes_namespace }}) (> 90%) VALUE = {{ $value }}\n"
      - alert: RedisTooManyConnections
        expr: redis_connected_clients / redis_config_maxclients * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Redis too many connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis is running out of connections (> 90% used) VALUE = {{ $value }}\n"
          resolved: "Redis connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisNotEnoughConnections
        expr: redis_connected_clients < 4
        for: 2m
        labels:
          severity: warning 
        annotations:
          summary: "Redis not enough connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Redis instance should have more connections (> 4) VALUE = {{ $value }}\n"
          resolved: "Redis enough connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
      - alert: RedisRejectedConnections
        expr: increase(redis_rejected_connections_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis rejected connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }})\n"
          description: "Some connections to Redis has been rejected on (namespace {{ $labels.kubernetes_namespace }}) VALUE = {{ $value }}\n"
          resolved: "Redis rejected connections (instance {{ $labels.instance }}) on (namespace {{ $labels.kubernetes_namespace }}) is resolved"
    - name: KubestateExporter
      rules:
      - alert: Deployment at 0 Replicas
        annotations:
          summary: "Deployment {{$labels.deployment}} is currently having no pods running\n"
          description: "Cluster Name: {{$externalLabels.cluster}}Namespace: {{$labels.namespace}}Deployment name: {{$labels.deployment}}\n" 
          resolved: "Deployment {{$labels.deployment}} is resolved"
        expr: |
          sum(kube_deployment_status_replicas{pod_template_hash=""}) by (deployment,namespace)  < 1
        for: 1m
        labels:
          severity: critical
      - alert: statefulset at 0 Replicas
        annotations:
          summary: "statefulset {{$labels.statefulset}} is currently having no pods running\n"
          description: "Cluster Name: {{$externalLabels.cluster}}Namespace: {{$labels.namespace}}statefulset name: {{$labels.statefulset}}\n" 
          resolved: "statefulset {{$labels.statefulset}} is resolved"
        expr: |
          sum(kube_statefulset_status_replicas{pod_template_hash=""}) by (statefulset,namespace)  < 1
        for: 1m
        labels:
          severity: critical
      - alert: KubernetesNodeunReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Node unready (instance {{ $labels.instance }})\n"
          description: "Node {{ $labels.node }} has been unready for a long time VALUE = {{ $value }}\n"
          resolved: "Kubernetes Node (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes memory pressure (instance {{ $labels.instance }})\n"
          description: "{{ $labels.node }} has MemoryPressure condition VALUE = {{ $value }}\n"
          resolved: "Kubernetes memory pressure (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes disk pressure (instance {{ $labels.instance }})\n"
          description: "{{ $labels.node }} has DiskPressure condition VALUE = {{ $value }}\n"
          resolved: "Kubernetes disk pressure (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesNetworkUnavailable
        expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes network unavailable (instance {{ $labels.instance }})\n"
          description: "{{ $labels.node }} has NetworkUnavailable condition VALUE = {{ $value }}\n"
          resolved: "Kubernetes network  (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesOutOfCapacity
        expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes out of capacity (instance {{ $labels.instance }})\n"
          description: "{{ $labels.node }} is out of capacity VALUE = {{ $value }}\n"
          resolved: "Kubernetes capacity  (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesContainerOomKiller
        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes container oom killer (instance {{ $labels.instance }})\n"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes. VALUE = {{ $value }}\n"
          resolved: "Kubernetes container  (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesJobFailed
        expr: kube_job_status_failed > 0
        for: 0m
        labels:
          severity: warning 
        annotations:
          summary: "Kubernetes Job failed (instance {{ $labels.instance }})\n"
          description: "Job {{ $labels.namespace }}/{{ $labels.exported_job }} failed to complete VALUE = {{ $value }}\n"
          resolved: "Kubernetes Job (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesCronjobSuspended
        expr: kube_cronjob_spec_suspend != 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})\n"
          description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended VALUE = {{ $value }}\n"
          resolved: "Kubernetes CronJob (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesPersistentvolumeclaimPending
        expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n"
          description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending VALUE = {{ $value }}\n"
          resolved: "Kubernetes PersistentVolumeClaim (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesVolumeOutOfDiskSpace
        expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning 
        annotations:
          summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})\n"
          description: "Volume is almost full (< 10% left) VALUE = {{ $value }}\n"
          resolved: "Kubernetes disk space (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesPersistentvolumeError
        expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})\n"
          description: "Persistent volume is in bad state VALUE = {{ $value }}\n"
          resolved: "Kubernetes PersistentVolume (instance {{ $labels.instance }}) is resolved and available"
      - alert: KubernetesStatefulsetDown
        expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0
        for: 1m
        labels:
          severity: critical 
        annotations:
          summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})\n"
          description: "{{ $labels.statefulset }} on  {{ $labels.namespace }}  StatefulSet went down VALUE = {{ $value }}\n"
          resolved: "Kubernetes StatefulSet (instance {{ $labels.instance }}) is resolved and up"
      - alert: KubernetesHpaScalingAbility
        expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})\n"
          description: "Pod is unable to scale VALUE = {{ $value }}\n"
          resolved: "Kubernetes HPA scaling (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesHpaMetricAvailability
        expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes HPA metric availability (instance {{ $labels.instance }})\n"
          description: "HPA is not able to collect metrics VALUE = {{ $value }}\n"
          resolved: "Kubernetes HPA metric  (instance {{ $labels.instance }}) is resolved and availabile"
      #  - alert: KubernetesHpaScaleCapability
      #    expr:  sum(kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas) by (cluster,namespace,horizontalpodautoscaler,instance)     #kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas
      #    for: 2m
      #    labels:
      #      severity: info
      #    annotations:
      #      summary: "Kubernetes HPA scale capability (name {{ $labels.horizontalpodautoscaler }} on namespace {{ $labels.namespace }})\n"
      #      description: "The maximum number of desired Pods has been hit VALUE = {{ $value }}\n"
      #  - alert: KubernetesHpaUnderutilized
      #    expr: max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3
      #    for: 0m
      #    labels:
      #      severity: info
      #    annotations:
      #      summary: "Kubernetes HPA underutilized (instance {{ $labels.instance }})\n"
      #      description: "HPA is constantly at minimum replicas for 50% of the time. Potential cost saving here. VALUE = {{ $value }}\n"
      - alert: KubernetesPodNotHealthy
        expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})\n"
          description: "Pod {{ $labels.pod }} on {{ $labels.namespace }} has been in a non-ready state for longer than 15 minutes. VALUE = {{ $value }}\n"
          resolved: "Kubernetes Pod (instance {{ $labels.instance }}) is resolved and healthy"
      - alert: KubernetesPodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})\n"
          description: "Pod {{ $labels.pod }} is crash looping VALUE = {{ $value }}\n"
          resolved: "Kubernetes Pod (instance {{ $labels.instance }}) is resolved and healthy"
      - alert: KubernetesReplicassetMismatch
        expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})  (replicaset {{ $labels.replicaset }}) (namespace {{ $labels.namespace }}) \n"
          description: "Deployment Replicas (replicaset {{ $labels.replicaset }}) (namespace {{ $labels.namespace }})  mismatch VALUE = {{ $value }}\n"
          resolved: "Kubernetes ReplicasSet  (instance {{ $labels.instance }})  (replicaset {{ $labels.replicaset }})  on (namespace {{ $labels.namespace }}) is resolved and healthy"
      - alert: KubernetesDeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})\n"
          description: "{{ $labels.deployment }} on  {{ $labels.namespace }}  Deployment Replicas mismatch VALUE = {{ $value }}\n"
          resolved: "Kubernetes Deployment replicas (instance {{ $labels.instance }})  is resolved"
      - alert: KubernetesStatefulsetReplicasMismatch
        expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})\n"
          description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet does not match the expected number of replicas. VALUE = {{ $value }}\n"
          resolved: "Kubernetes StatefulSet  replicas (instance {{ $labels.instance }})  is resolved"
      - alert: KubernetesDeploymentGenerationMismatch
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})\n"
          description: "{{ $labels.deployment }} on  {{ $labels.namespace }} Deployment has failed but has not been rolled back. VALUE = {{ $value }}\n"
          resolved: "Kubernetes Deployment generation (instance {{ $labels.instance }})  is resolved"
      - alert: KubernetesStatefulsetGenerationMismatch
        expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})\n"
          description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet has failed but has not been rolled back. VALUE = {{ $value }}\n"
          resolved: "Kubernetes StatefulSet generation (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesStatefulsetUpdateNotRolledOut
        expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})\n"
          description: "{{ $labels.statefulset }} on  {{ $labels.namespace }} StatefulSet update has not been rolled out. VALUE = {{ $value }}\n"
          resolved: "Kubernetes StatefulSet (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesDaemonsetRolloutStuck
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})\n"
          description: "Some Pods of DaemonSet are not scheduled or not ready VALUE = {{ $value }}\n"
          resolved: "Kubernetes DaemonSet (instance {{ $labels.instance }}) is resolved"
      - alert: KubernetesDaemonsetMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})\n"
          description: "Some DaemonSet Pods are running where they are not supposed to run VALUE = {{ $value }}\n"
          resolved: "Kubernetes DaemonSet (instance {{ $labels.instance }}) is resolved"
      #- alert: KubernetesCronjobTooLong
      #  expr: 'time() - kube_cronjob_next_schedule_time > 3600'
      #  for: 0m
      #  labels:
      #    severity: warning
      #  annotations:
      #    summary: "Kubernetes CronJob too long (instance {{ $labels.instance }})\n"
      #    description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete. VALUE = {{ $value }}\n"
      - alert: KubernetesJobSlowCompletion
        expr: 'kube_job_spec_completions - kube_job_status_succeeded > 0'
        for: 12h
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes job slow completion (instance {{ $labels.instance }})\n"
          description: "Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time. VALUE = {{ $value }}\n"
          resolved: "Kubernetes job  (instance {{ $labels.instance }}) is resolved"
    - name: Argocd_alerts
      rules:
      - alert: ArgocdServiceNotSynced
        expr: sum(argocd_app_info{sync_status!="Synced",name!="dartil-ing-disable-test",name!="dartil-ing-disable-develop",name!="dartil-ing-disable-stage"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type) #argocd_app_info{sync_status!="Synced",name!="dartil-ing-disable-test",name!="dartil-ing-disable-develop"} != 0
        for: 5m
        labels:
          severity: warning
          type: argocd-metrics
        annotations:
          summary: "ArgoCD service not synced (instance {{ $labels.name }})\n"
          description: "Service {{ $labels.name }} run by argo is currently not in sync. \n"
          resolved: "ArgoCD service {{ $labels.name }} is resolved"
      - alert: ArgocdServiceUnknown
        expr: sum( argocd_app_info{sync_status="Unknown"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type) #argocd_app_info{sync_status="Unknown"} != 0
        for: 5m
        labels:
          severity: warning
          type: argocd-metrics
        annotations:
          summary: "ArgoCD service Unknown (instance {{ $labels.name }})\n"
          description: "Service {{ $labels.name }} run by argo is currently in Unknown. \n"
          resolved: "ArgoCD service {{ $labels.name }} is resolved"
      - alert: ArgocdServiceUnhealthy
        expr: sum(argocd_app_info{health_status!="Healthy",health_status!="Progressing"} != 0) by (name,autosync_enabled,dest_namespace,health_status,project,severity,sync_status,type)
        for: 5m
        labels:
          severity: warning
          type: argocd-metrics
        annotations:
          summary: "ArgoCD service unhealthy (instance {{ $labels.name }})\n"
          description: "Service {{ $labels.name }} run by argo is currently not healthy. \n"
          resolved: "ArgoCD service {{ $labels.name }} is resolved"
    - name: MSSQL_alerts
      rules:
      - alert: SqlServerDeadlock
        expr: increase(mssql_deadlocks[3m]) > 2
        for: 0m
        labels:
          severity: warning
          type: mssql-metrics
        annotations:
          summary: "SQL Server deadlock (instance {{ $labels.instance }})\n"
          description: "SQL Server is having some deadlock. VALUE = {{ $value }}\n"
          resolved: "SQL Server deadlock (instance {{ $labels.instance }}) is resolved"
    - name: BlackBox_alerts
      rules:
      #  - alert: BlackboxProbeFailed
      #    expr: probe_success{} == 0 
      #    for: 1m
      #    labels:
      #      severity: critical
      #      type: blackbox_metrics
      #    annotations:
      #      summary: "Blackbox probe failed (instance {{ $labels.instance }}) on  job {{ $labels.job }}\n"
      #      description: "Probe failed  VALUE = {{ $value }}\n"
      #      resolved: "Blackbox probe (instance {{ $labels.instance }}) on  job {{ $labels.job }} is resolved"
      - alert: BlackboxSlowProbe
        expr: avg_over_time(probe_duration_seconds[1m]) > 15
        for: 1m
        labels:
          severity: warning
          type: blackbox_metrics
        annotations:
          summary: "Blackbox slow probe (instance {{ $labels.instance }}) on  job {{ $labels.job }} \n"
          description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n "
          resolved: "Blackbox slow probe (instance {{ $labels.instance }}) on  job {{ $labels.job }} is resolved"
      - alert: BlackboxProbeHttpFailure
        expr: probe_http_status_code <= 199 OR probe_http_status_code >= 499
        for: 1m
        labels:
          severity: critical
          type: blackbox_metrics
        annotations:
          summary: "Blackbox probe HTTP failure (instance {{ $labels.instance }}) on job {{ $labels.job }} \n"
          description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n"
          resolved: "Blackbox probe HTTP (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
      - alert: BlackboxProbeTransferFailure
        expr: probe_http_duration_seconds{phase="processing"} == 0
        for: 1m
        labels:
          severity: critical
          type: blackbox_metrics
        annotations:
          summary: "Blackbox probe Transfer failure (instance {{ $labels.instance }}) on job {{ $labels.job }}"
          description: Blackbox probe Transfer for (instance {{ $labels.instance }}) is Zero"       
          resolved: "Blackbox probe Transfer (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
      - alert: BlackboxSslCertificateWillExpireSoon
        expr: 3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20
        for: 0m
        labels:
          severity: warning
          type: blackbox_metrics
        annotations:
          summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }}) on job {{ $labels.job }}\n"
          description: "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}\n" 
          resolved: "Blackbox  SSL certificate (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
      - alert: BlackboxProbeSlowHttp
        expr: avg_over_time(probe_http_duration_seconds[1m]) > 15
        for: 1m
        labels:
          severity: warning
          type: blackbox_metrics
        annotations:
          summary: "Blackbox probe slow HTTP (instance {{ $labels.instance }}) on job {{ $labels.job }}\n"
          description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n" 
          resolved: "Blackbox probe slow HTTP (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
      - alert: BlackboxProbeSlowPing
        expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 5
        for: 1m
        labels:
          severity: warning
          type: blackbox_metrics
        annotations:
          summary: "Blackbox probe slow ping (instance {{ $labels.instance }})\n"
          description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n"     
          resolved: "Blackbox probe ping (instance {{ $labels.instance }}) on job {{ $labels.job }} is resolved"
    - name: elasticsearch
      rules:
      - alert: ElasticsearchHeapUsageTooHigh
        expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
        for: 2m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Heap Usage Too High (name {{ $labels.name }})\n"
          description: "The heap usage is over 90%  VALUE = {{ $value }}\n"
          resolved: "Elasticsearch Heap Usage (name {{ $labels.name }}) is resolved"
      - alert: ElasticsearchHeapUsageWarning
        expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
        for: 2m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Heap Usage warning (name {{ $labels.name }})\n"
          description: "The heap usage is over 80% VALUE = {{ $value }}\n"
          resolved: "Elasticsearch Heap Usage (name {{ $labels.name }}) is resolved"
      - alert: ElasticsearchDiskOutOfSpace
        expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10
        for: 0m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch disk out of space (name {{ $labels.name }})\n"
          description: "The disk usage is over 90% VALUE = {{ $value }}\n"
          resolved: "Elasticsearch disk space (name {{ $labels.name }}) is resolved"
      - alert: ElasticsearchDiskSpaceLow
        expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20
        for: 2m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch disk space low (name {{ $labels.name }})\n"
          description: "The disk usage is over 80% VALUE = {{ $value }}\n"
          resolved: "Elasticsearch disk space (name {{ $labels.name }}) is resolved"
      - alert: ElasticsearchClusterRed
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 0m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Cluster Red (instance {{ $labels.instance }})\n"
          description: "Elastic Cluster Red status VALUE = {{ $value }}\n"
          resolved: "Elasticsearch Cluster (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchClusterYellow
        expr: elasticsearch_cluster_health_status{color="yellow"} == 1
        for: 0m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Cluster Yellow (instance {{ $labels.instance }})\n"
          description: "Elastic Cluster Yellow status VALUE = {{ $value }}\n"
          resolved: "Elasticsearch Cluster (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchHealthyNodes
        expr: elasticsearch_cluster_health_number_of_nodes < 3
        for: 0m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Healthy Nodes (instance {{ $labels.instance }})\n"
          description: "Missing node in Elasticsearch cluster VALUE = {{ $value }}\n"
          resolved: "Elasticsearch nodes (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchHealthyDataNodes
        expr: elasticsearch_cluster_health_number_of_data_nodes < 3
        for: 0m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch Healthy Data Nodes (instance {{ $labels.instance }})\n"
          description: "Missing data node in Elasticsearch cluster VALUE = {{ $value }}\n"
          resolved: "Elasticsearch data nodes (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchRelocatingShards
        expr: elasticsearch_cluster_health_relocating_shards > 0
        for: 0m
        labels:
          severity: info
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch relocating shards (instance {{ $labels.instance }})\n"
          description: "Elasticsearch is relocating shards VALUE = {{ $value }}\n"
          resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"
      - alert: ElasticsearchRelocatingShardsTooLong
        expr: elasticsearch_cluster_health_relocating_shards > 0
        for: 15m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch relocating shards too long (instance {{ $labels.instance }})\n"
          description: "Elasticsearch has been relocating shards for 15min VALUE = {{ $value }}\n"
          resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"
      - alert: ElasticsearchInitializingShards
        expr: elasticsearch_cluster_health_initializing_shards > 0
        for: 0m
        labels:
          severity: info
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch initializing shards (instance {{ $labels.instance }})\n"
          description: "Elasticsearch is initializing shards VALUE = {{ $value }}\n"
          resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"
      - alert: ElasticsearchInitializingShardsTooLong
        expr: elasticsearch_cluster_health_initializing_shards > 0
        for: 15m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch initializing shards too long (instance {{ $labels.instance }})\n"
          description: "Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }}\n"
          resolved: "Elasticsearch relocating shards (instance {{ $labels.instance }}) is ended"
      - alert: ElasticsearchUnassignedShards
        expr: elasticsearch_cluster_health_unassigned_shards > 0
        for: 0m
        labels:
          severity: critical
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch unassigned shards (instance {{ $labels.instance }})\n"
          description: "Elasticsearch has unassigned shards VALUE = {{ $value }}\n"
          resolved: "Elasticsearch unassigned shards (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchPendingTasks
        expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
        for: 15m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch pending tasks (instance {{ $labels.instance }})\n"
          description: "Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }}\n"
          resolved: "Elasticsearch pending tasks (instance {{ $labels.instance }}) is resolved"
      - alert: ElasticsearchNoNewDocuments
        expr: increase(elasticsearch_indices_indexing_index_total{es_data_node="true"}[10m]) < 1
        for: 0m
        labels:
          severity: warning
          type: monitoring-metrics
        annotations:
          summary: "Elasticsearch no new documents (instance {{ $labels.instance }})\n"
          description: "No new documents for 10 min! VALUE = {{ $value }}\n"
          resolved: "Elasticsearch documents  (instance {{ $labels.instance }}) is resolved"

  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
  alerts: {}

  ## Records configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
  recording_rules.yml: {}
  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
  rules: {}

  prometheus.yml:
    rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
    ## Below two files are DEPRECATED will be removed from this default values file
      - /etc/config/rules
      - /etc/config/alerts

    scrape_configs:
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090

      # A scrape configuration for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.

      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: 'kubernetes-apiservers'

        kubernetes_sd_configs:
          - role: endpoints

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics


      - job_name: 'kubernetes-nodes-cadvisor'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        # This configuration will work only on kubelet 1.7.3+
        # As the scrape endpoints for cAdvisor have changed
        # if you are using older version you need to change the replacement to
        # replacement: /api/v1/nodes/$1:4194/proxy/metrics
        # more info here https://github.com/coreos/prometheus-operator/issues/633
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

        # Metric relabel configs to apply to samples before ingestion.
        # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
        # metric_relabel_configs:
        # - action: labeldrop
        #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of
      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
      # then you can set any parameter
      - job_name: 'kubernetes-service-endpoints'
        honor_labels: true

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
            action: drop
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: service
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

      # Scrape config for slow service endpoints; same as above, but with a larger
      # timeout and a larger interval
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
      # then you can set any parameter
      - job_name: 'kubernetes-service-endpoints-slow'
        honor_labels: true

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: service
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

      - job_name: 'prometheus-pushgateway'
        honor_labels: true

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: pushgateway

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: 'kubernetes-services'
        honor_labels: true

        metrics_path: /probe
        params:
          module: [http_2xx]

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,
      # except if `prometheus.io/scrape-slow` is set to `true` as well.
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'
        honor_labels: true

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
            action: drop
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            regex: (https?)
            target_label: __scheme__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
            action: replace
            regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
            replacement: '[$2]:$1'
            target_label: __address__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
            action: replace
            regex: (\d+);((([0-9]+?)(\.|$)){4})
            replacement: $2:$1
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_phase]
            regex: Pending|Succeeded|Failed|Completed
            action: drop
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

      # Example Scrape config for pods which should be scraped slower. An useful example
      # would be stackriver-exporter which queries an API on every scrape of the pod
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods-slow'
        honor_labels: true

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            regex: (https?)
            target_label: __scheme__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
            action: replace
            regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
            replacement: '[$2]:$1'
            target_label: __address__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
            action: replace
            regex: (\d+);((([0-9]+?)(\.|$)){4})
            replacement: $2:$1
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_phase]
            regex: Pending|Succeeded|Failed|Completed
            action: drop
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

# adds additional scrape configs to prometheus.yml
# must be a string so you have to add a | after extraScrapeConfigs:
# example adds prometheus-blackbox-exporter scrape config
extraScrapeConfigs: ""
  # - job_name: 'prometheus-blackbox-exporter'
  #   metrics_path: /probe
  #   params:
  #     module: [http_2xx]
  #   static_configs:
  #     - targets:
  #       - https://example.com
  #   relabel_configs:
  #     - source_labels: [__address__]
  #       target_label: __param_target
  #     - source_labels: [__param_target]
  #       target_label: instance
  #     - target_label: __address__
  #       replacement: prometheus-blackbox-exporter:9115

# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager
# useful in H/A prometheus with different external labels but the same alerts
alertRelabelConfigs: {}
  # alert_relabel_configs:
  # - source_labels: [dc]
  #   regex: (.+)\d+
  #   target_label: dc

networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false

# Force namespace of namespaced resources
forceNamespace: ""

# Extra manifests to deploy as an array
extraManifests: []
  # - |
  #   apiVersion: v1
  #   kind: ConfigMap
  #   metadata:
  #   labels:
  #     name: prometheus-extra
  #   data:
  #     extra-data: "value"

# Configuration of subcharts defined in Chart.yaml

## alertmanager sub-chart configurable values
## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager
##
alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: false

  persistence:
    size: 2Gi

  podSecurityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534

## kube-state-metrics sub-chart configurable values
## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
##
kube-state-metrics:
  ## If false, kube-state-metrics sub-chart will not be installed
  ##
  enabled: true

## prometheus-node-exporter sub-chart configurable values
## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
##
prometheus-node-exporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: true

  rbac:
    pspEnabled: false

  containerSecurityContext:
    allowPrivilegeEscalation: false

## prometheus-pushgateway sub-chart configurable values
## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway
##
prometheus-pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: false

  # Optional service annotations
  serviceAnnotations:
    prometheus.io/probe: pushgateway
